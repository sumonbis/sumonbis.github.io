<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Sumon Biswas</title>
    <link>https://sumonbis.github.io/post/</link>
      <atom:link href="https://sumonbis.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2021 Sumon Biswas</copyright><lastBuildDate>Sat, 06 Aug 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://sumonbis.github.io/media/icon_hub4cc8572db2cf4087d387aa00fd8abd9_555_512x512_fill_lanczos_center_3.png</url>
      <title>Posts</title>
      <link>https://sumonbis.github.io/post/</link>
    </image>
    
    <item>
      <title>SPLASH 2021 Retrospective: Organizing a Premier PL Conference in Hybrid</title>
      <link>https://sumonbis.github.io/post/splash/</link>
      <pubDate>Sat, 06 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/post/splash/</guid>
      <description>&lt;p&gt;I served as an Accessibility Chair in the organizing committee of the &lt;a href=&#34;https://2020.splashcon.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPLASH conference in 2020&lt;/a&gt;.
SPLASH is the premier conference on the applications of programming languages- at the intersection of programming languages and software engineering. It hosted the &lt;strong&gt;OOPSLA, ECOOP, and other co-located events&lt;/strong&gt;. Because of the pandemic, SPLASH held in 2020 into a virtual setting.
I facilitated the audio-video (AV) accessibility support for the authors and audiences. Especially, I enabled closed-captions for all the talks in the conference for their talks for better accessibility. I provided instructions and tutorials to all the authors for creation of closed captions of their talks. Despite several challenges of correction and synchronization, I was able to provide accessibility support to the conference.&lt;/p&gt;
&lt;p&gt;The success of virtualization of SPLASH 2020 was recognized and appreciated by the organizers, and in the following year I served as the Accessibility Chair in SPLASH 2021 as well. This time &lt;a href=&#34;https://2021.splashcon.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPLASH 2021&lt;/a&gt; was hybrid with close to three hundred in-person participation in Chicago and over four hundred virtual attendees. I observed a great positivity and inclusive culture in the conference which was possible through an easy-to-access medium of operation and technical support. The hybrid conference also showed the benefits of inclusivity in a great deal. It showed that even after pandemic, if we have a hybrid setting in such conferences, a lot more people can join and interact internationally, which is often very limited from many underrepresented country. For example, a student from India could not have traveled to Chicago because of several barriers like visa, travel-cost, etc. However, these people are a big part of the community in terms of publications and representation. The hybrid event gives almost a seamless experience for their participation. Albeit a lot to improve for the hybridization, we wrote a SIGPLAN blog on the hybrid conference organization in 2021.&lt;/p&gt;
&lt;p&gt;Me along with Jonathan Aldrich, Steve Blackburn, Benjamin Chung, Youyou Cong, Alex Potanin, Hridesh Rajan, and Talia Ringer, shared our experience in this &lt;a href=&#34;https://blog.sigplan.org/2022/08/25/hybrid-splash-2021-retrospective/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SIGPLAN blog&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Our Research Identifies Unfairness in the Component Level of AI Based Software</title>
      <link>https://sumonbis.github.io/post/fair-pipeline/</link>
      <pubDate>Sun, 02 May 2021 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/post/fair-pipeline/</guid>
      <description>&lt;p&gt;The government, academia, industry are increasingly employing artificial intelligence (AI) systems in decision making. With the availability of numerous data, AI systems are becoming more popular in various sectors. Many of these systems affect human lives directly in one way or another. Our research highlighted that many of such real-world machine learning (ML) models exhibit unfairness with respect to certain societal groups of race, sex or age. In the last few years, our software design lab employed significant effort to identify fairness in machine learning algorithms and mitigate that effectively. Recent result shows that several components in an ML pipeline are influencing the predictive result that is unfair to minority groups such as dark-skinned people or female.&lt;/p&gt;
&lt;p&gt;I and my advisor Hridesh Rajan are working in the D4 Institute at Iowa State which broadly focuses on increasing the dependability of AI-based systems.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The accuracy of a model is not always telling the whole story. How much bias the model propagates or how much we can trust the prediction is a big question.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;These AI-based software are being used in hiring employees, approving loans, criminal sentencing, which should be more accountable and explainable. Analyzing the behavior of ML pipeline in component level would help towards that goal.
&lt;a href=&#34;https://sumonbis.github.io/publication/esec-fse21/&#34;&gt;Our paper proposing a novel method to identify unfairness of ML components&lt;/a&gt; has been recently accepted in &lt;a href=&#34;https://2021.esec-fse.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ESEC/FSE 2021&lt;/a&gt;, which is one of top software engineering conference and internationally renowned forum for researchers, practitioners, and educators.&lt;/p&gt;
&lt;p&gt;In the paper, we proposed a causal method to measure the fairness of different stages in ML pipeline.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Although some recent work proposed metrics to quantify the bias in the predictions, ML software is complex having several stages. Our method could effectively identify the data transformers that caused unfairness in such software.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We strongly believe that the researchers and practitioners would be able to leverage our approach to avoid biased data preprocessing. Our goal in the long-run would be to unveil the ML black box and reason about fairness constraints.&lt;/p&gt;
&lt;p&gt;I will present the results of the paper entitled “Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline”, in the research track of the 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) to be held in Athens, Greece from August 23-28, 2021. The preprint of the paper is available here: &lt;a href=&#34;https://arxiv.org/pdf/2106.06054.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/2106.06054.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We are excited on the acceptance of the paper in a top venue in our area. We are continuing the research to explore new avenues and assure the fairness of machine learning software.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to Add Closed Captions in Your Video Talk</title>
      <link>https://sumonbis.github.io/post/closed-captions/</link>
      <pubDate>Tue, 13 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/post/closed-captions/</guid>
      <description>&lt;p&gt;YouTube Studio can be used to generate closed captions for your talk by following these simple steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;First, record the video of the talk. The video should have a clear voice recording.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sign in to &lt;a href=&#34;https://studio.youtube.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YouTube Studio&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click on &lt;code&gt;CREATE &amp;gt; Upload videos&lt;/code&gt;. Then upload the recorded talk and fill the standard options (title, description, etc.). Finish the upload and publish it. It will take some time to upload and process the video.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- 













&lt;figure  id=&#34;figure-upload-video-on-youtube-studio&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Upload video on YouTube Studio&#34; srcset=&#34;
               /post/closed-captions/upload_hu08fd69e9e84e7425239b423688b7ff4a_12001_14a869470a1b89444bd8053e0b9be2c5.png 400w,
               /post/closed-captions/upload_hu08fd69e9e84e7425239b423688b7ff4a_12001_bacc2bca902070aa18ce762b949e71d9.png 760w,
               /post/closed-captions/upload_hu08fd69e9e84e7425239b423688b7ff4a_12001_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/post/closed-captions/upload_hu08fd69e9e84e7425239b423688b7ff4a_12001_14a869470a1b89444bd8053e0b9be2c5.png&#34;
               width=&#34;331&#34;
               height=&#34;93&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Upload video on YouTube Studio
    &lt;/figcaption&gt;&lt;/figure&gt; --&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    To be able to auto-generate the captions, select the &lt;code&gt;Video language&lt;/code&gt; (e.g., English), and &lt;code&gt;Caption certification&lt;/code&gt; (e.g., This content has never aired on television in the U.S.).
  &lt;/div&gt;
&lt;/div&gt;














&lt;figure  id=&#34;figure-steps-for-generating-captions-in-youtube-studio&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Steps for generating captions in YouTube Studio&#34;
           src=&#34;https://sumonbis.github.io/post/closed-captions/process.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Steps for generating captions in YouTube Studio
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;After the video is published, select &lt;code&gt;Videos&lt;/code&gt; from the left menu and click on the video. It will open the video editing options.&lt;/li&gt;
&lt;/ol&gt;














&lt;figure  id=&#34;figure-video-editing-options&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Video editing options&#34; srcset=&#34;
               /post/closed-captions/editing_hudc065b6217f6da97f5e9c3d73465d578_22345_3d670f216c10cc93f023eaaf0526bb8b.png 400w,
               /post/closed-captions/editing_hudc065b6217f6da97f5e9c3d73465d578_22345_b48179162dd89ab02128900adb791046.png 760w,
               /post/closed-captions/editing_hudc065b6217f6da97f5e9c3d73465d578_22345_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/post/closed-captions/editing_hudc065b6217f6da97f5e9c3d73465d578_22345_3d670f216c10cc93f023eaaf0526bb8b.png&#34;
               width=&#34;760&#34;
               height=&#34;115&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Video editing options
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Click &lt;code&gt;Subtitles&lt;/code&gt; from the left menu. You should see &lt;code&gt;English(Automatic)&lt;/code&gt; subtitle option there. Click &lt;code&gt;DUPLICATE AND EDIT&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    This option might not be available immediately after uploading a video. For me, YouTube Studio took around half an hour to make the subtitle available for a 15-min video.
  &lt;/div&gt;
&lt;/div&gt;














&lt;figure  id=&#34;figure-generate-subtitles&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Generate subtitles&#34; srcset=&#34;
               /post/closed-captions/subtitles_hu497f034dd612990d1e15ac6f0579d693_42581_e90dbf9fa7cfe5ec320ad045a2288857.png 400w,
               /post/closed-captions/subtitles_hu497f034dd612990d1e15ac6f0579d693_42581_5af088a46693f0902883d5f0a5929749.png 760w,
               /post/closed-captions/subtitles_hu497f034dd612990d1e15ac6f0579d693_42581_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/post/closed-captions/subtitles_hu497f034dd612990d1e15ac6f0579d693_42581_e90dbf9fa7cfe5ec320ad045a2288857.png&#34;
               width=&#34;760&#34;
               height=&#34;212&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Generate subtitles
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;You should see the automatically generated text from the speech. You have two options to finalize the captions:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;You can edit the text to make corrections: adding punctuations, capitalization, misspelled words, etc. The text will be automatically synced with the timing. However, you can modify the timings too. Note that, in one timeframe, you should not put a lot of characters which will cover much space on the screen.&lt;/li&gt;
&lt;li&gt;Alternatively, you can delete the automatically generated text and add your text manually by clicking on &lt;code&gt;+CAPTION&lt;/code&gt;. Then write the text, start time, and end time.&lt;/li&gt;
&lt;/ul&gt;














&lt;figure  id=&#34;figure-edit-subtitles&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Edit subtitles&#34; srcset=&#34;
               /post/closed-captions/timing_hu8393fbf032e093c702c75fdff53cba05_64692_2221ea3bec630bb61bd1f5818125c1bf.png 400w,
               /post/closed-captions/timing_hu8393fbf032e093c702c75fdff53cba05_64692_1c789b521e0ff1e46e3d1ffaa14ec2cc.png 760w,
               /post/closed-captions/timing_hu8393fbf032e093c702c75fdff53cba05_64692_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/post/closed-captions/timing_hu8393fbf032e093c702c75fdff53cba05_64692_2221ea3bec630bb61bd1f5818125c1bf.png&#34;
               width=&#34;529&#34;
               height=&#34;308&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Edit subtitles
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;
&lt;p&gt;Save the draft and click &lt;code&gt;PUBLISH&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, click on &lt;code&gt;Options&lt;/code&gt; beside the &lt;code&gt;EDIT&lt;/code&gt; button and click &lt;code&gt;Download&lt;/code&gt;. Then select &lt;code&gt;.srt&lt;/code&gt; format. Thus, you get your closed caption for your talk in a &lt;code&gt;.srt&lt;/code&gt; file.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    The &lt;code&gt;.srt&lt;/code&gt; file is editable with a text editor as well. Be careful with the formatting and finally check whether the captions are working on a video player on your computer.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- https://support.google.com/youtube/answer/2734796?hl=en#zippy= --&gt;
&lt;p&gt;Here is an &lt;a href=&#34;https://youtu.be/C7lfPoMbpIA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;example talk&lt;/a&gt; with closed captions. Click on the CC button on the video if the closed captions are not enabled on YouTube. You can get other details about YouTube Studio captioning &lt;a href=&#34;https://support.google.com/youtube/answer/2734796?hl=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;other-options&#34;&gt;Other Options&lt;/h2&gt;
&lt;p&gt;Apart from YouTube Studio, there are other tools too for generating captions (&lt;code&gt;.srt&lt;/code&gt;).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://maestrasuite.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Maestra&lt;/a&gt; is an online service to generate and export subtitles. You can sign in, upload your video talk, transcribe, edit the auto-generated subtitles, and then export &lt;code&gt;.srt&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://otter.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Otter.ai&lt;/a&gt; can be used to do the task. You can find the instructions &lt;a href=&#34;https://blog.otter.ai/video-captions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. Although its a paid service, there could be some free trials.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.rev.com/caption&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rev&lt;/a&gt; is another paid service to generate captions manually by human experts.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Other tools can be also used. Depending on the backend transcribe engine, the correctness can vary. However, you should edit the auto-generated text to finalize the closed captions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Being a Part of A Premier Data Science Research Hub</title>
      <link>https://sumonbis.github.io/post/d4-institute/</link>
      <pubDate>Sat, 15 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/post/d4-institute/</guid>
      <description>&lt;p&gt;D4 Institute is an interdisciplinary data science hub at Iowa State university where professors, graduate students, REU students, and researchers from Computer Science, Electrical Engineering, Mathematics, Statistics collaborate to ensure the dependability of data science.&lt;/p&gt;
&lt;p&gt;D4 institute took about four year to assemble and then funded by NSF TRIPOD grant in 2020. My advisor Hridesh Rajan leads the project as an PI. I have been involved with D4 from the beginning of writing the NSF proposal. Afterwards, I continue to contribute as a graduate researcher in the project.&lt;/p&gt;
&lt;p&gt;The computer science magazine Atanasoff Today featured out work in the D4 Institute. The magazine is available in here: &lt;a href=&#34;https://www.cs.iastate.edu/atanasoff-today-piecing-together-premier-data-science-research-hub&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cs.iastate.edu/atanasoff-today-piecing-together-premier-data-science-research-hub&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Sumon Biswas (’21 computer science, Ph.D.) was immediately drawn to Iowa State’s computer science program, in part because of Rajan’s group. Research opportunities related to the data science field matched nicely with his career goals. Biswas was particularly drawn to the group’s commitment to researching the data science pipeline.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The central goal of the project is to ensure the dependability of data-driven software. With the growing interests in AI and machine learning, we need to focus on the safety, security, fairness, robustness, and more critical properties of such systems.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“My research interests are very specific and tailored. My career focus blends software engineering, programming languages and data science,” Biswas said. “The varied research opportunities at Iowa State, in particular with the D4 Institute, allowed me to become an entrepreneur of sorts and design my own career that fit with my goals.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;My advisor guided through the process to delve into the area and make original contribution in the project. We are continuing to work in the area and blend the software engineering and programming language expertise to bring more reliability on the AI and ML based systems.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Rajan has provided Biswas with a rich array of opportunities that have shaped his career path. In addition to engaging in cutting-edge research on the data science life cycle, Biswas provided significant contributions to the development of the successful TRIPODS NSF grant. He also attended the Midwest Big Data Summer School where he learned cutting-edge research methods that further drew him into studying the data science life cycle.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Specifically, I looked deep into the data science pipeline, which is a ordered set of stages including data collection, exploratory analysis, data preprocessing, modeling, training, evaluation, and different properties of the pipeline.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“It’s been incredible,” Biswas said. “I’ve learned novel research ideas from D4 researchers and practitioners who have introduced me to studying the data science pipeline and its properties.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I already published my research work on ensuring fairness of machine learning models. The work analyzes different fairness measures, mitigation techniques, and their impacts in real-world ML based software.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Biswas is close to publishing his own research which he conducted at the D4 Institute. “It’s exciting to be involved in research that could improve software systems, which affect many people who are impacted by data-driven decisions,” he said.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Rajan and his team plan to hire additional undergraduates, graduate students and postdocs at the D4 Institute. More students, like Biswas, will benefit from the experience of conducting NSF-funded research and working with seasoned experts who collaborate on studies.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We have a full-grown team of collaborators now, undergraduate and graduate students, postdocs, industry partners, and professors from different expertise. I have also mentored undergraduate students and collaborated with others, which was a great experience to gain further knowledge, and share thoughts and ideas.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Boa for Big-Code Mining and Large-Scale Static Analysis</title>
      <link>https://sumonbis.github.io/post/boa-tutorial/</link>
      <pubDate>Fri, 13 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/post/boa-tutorial/</guid>
      <description>&lt;p&gt;This tutorial will describe how to create your own Boa dataset for specific projects in Github and run Boa queries on that dataset locally. We will use command line and Eclipse IDE for that purpose.&lt;/p&gt;
&lt;h2 id=&#34;prerequisite&#34;&gt;Prerequisite&lt;/h2&gt;
&lt;p&gt;You need to have following already installed in your system:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;JDK&lt;/li&gt;
&lt;li&gt;Apache Ant&lt;/li&gt;
&lt;li&gt;Git&lt;/li&gt;
&lt;li&gt;Eclipse IDE&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;development-setup-steps&#34;&gt;Development Setup Steps&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Clone the Boa project using the command line: &lt;code&gt;git clone https://github.com/boalang/compiler.git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Go to the cloned repository: &lt;code&gt;cd compiler&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Clean the project: &lt;code&gt;ant clean&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create a directory for libraries: &lt;code&gt;mkdir -p build/classes&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Compile the project: &lt;code&gt;ant compile&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create a class folder: &lt;code&gt;mkdir compile&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;In Eclipse go to: File &amp;gt; Open Projects from File System &amp;gt; Import Source – Directory &amp;gt; Browse the cloned repository (compiler) &amp;gt; Hit Finish&lt;/li&gt;
&lt;/ol&gt;














&lt;figure  id=&#34;figure-import-compiler-project-in-eclipse&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Import compiler project in Eclipse&#34; srcset=&#34;
               /post/boa-tutorial/import_hu18ade01ed00817a4bb1fa04cbf19de50_100306_8b8e2b26342e735bcc4f07d6f49124d5.png 400w,
               /post/boa-tutorial/import_hu18ade01ed00817a4bb1fa04cbf19de50_100306_36ddbde6110206d333a9f39ed372eb8e.png 760w,
               /post/boa-tutorial/import_hu18ade01ed00817a4bb1fa04cbf19de50_100306_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/post/boa-tutorial/import_hu18ade01ed00817a4bb1fa04cbf19de50_100306_8b8e2b26342e735bcc4f07d6f49124d5.png&#34;
               width=&#34;760&#34;
               height=&#34;529&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Import compiler project in Eclipse
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;!-- &lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;img/import.png&#34; title=&#34;Import compiler project in Eclipse&#34;&gt;
&lt;/p&gt; --&gt;
&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;Right click on the project compiler &amp;gt; Build Path &amp;gt; Configure Build Path&lt;/li&gt;
&lt;li&gt;In Source tab, click Add Folder to add the required source folders and remove unnecessary folder(s). After adding all the folders, the window should look like the following:&lt;/li&gt;
&lt;/ol&gt;














&lt;figure  id=&#34;figure-after-configuring-eclipse&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;After configuring Eclipse&#34; srcset=&#34;
               /post/boa-tutorial/after_config_huc30bb9b6770fa560d57158247648e19e_254071_0f267088009d39f4d6144eed5cf2e29e.png 400w,
               /post/boa-tutorial/after_config_huc30bb9b6770fa560d57158247648e19e_254071_8beb014657c4a32573ee7449d578aefd.png 760w,
               /post/boa-tutorial/after_config_huc30bb9b6770fa560d57158247648e19e_254071_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/post/boa-tutorial/after_config_huc30bb9b6770fa560d57158247648e19e_254071_0f267088009d39f4d6144eed5cf2e29e.png&#34;
               width=&#34;760&#34;
               height=&#34;438&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      After configuring Eclipse
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;!-- &lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;img/after_config.png&#34; title=&#34;After configuring Eclipse&#34;&gt;
&lt;/p&gt; --&gt;
&lt;ol start=&#34;10&#34;&gt;
&lt;li&gt;Select Libraries tab in the same window and click on Add Class Folder and add the compiler/compile folder that has been created in step 6.&lt;/li&gt;
&lt;li&gt;Click Add JARs… in the same Libraries tab &amp;gt; select lib &amp;gt; select all the files inside lib, including files under datagen and evaluator folder &amp;gt; hit Apply and Close.&lt;/li&gt;
&lt;li&gt;From the compiler project in Eclipse, right click on build.xml &amp;gt; Run as &amp;gt; 1 Ant build. This should build the project successfully. The development setup is completed. Now, we will move on to data generation steps.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;boa-data-generation-steps&#34;&gt;Boa Data Generation Steps&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Go to github.com and search the project for which you want to create the dataset. For example, if you want to create dataset for Apache Mavan project, go to &lt;a href=&#34;https://github.com/apache/maven&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/apache/maven&lt;/a&gt; .&lt;/li&gt;
&lt;li&gt;Invoke a GitHub http-based RESTful API to get the metadata of the project by constructing a URL &lt;a href=&#34;https://api.github.com/repos/repo_full_name&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://api.github.com/repos/repo_full_name&lt;/a&gt;, for example &lt;a href=&#34;https://api.github.com/repos/apache/maven&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://api.github.com/repos/apache/maven&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Copy the whole JSON metadata, create a blank text file, type a pair of brackets ‘[]’, paste the metadata inside the brackets.&lt;/li&gt;
&lt;li&gt;Search for &amp;ldquo;languages_url&amp;rdquo; field in the JSON data, go to the URL, copy everything, create another field in the JSON file (&amp;ldquo;language_list&amp;rdquo;: ), paste copied text and save the file as filename.json (e.g., maven.json). The last few lines of the JSON file should look like:&lt;/li&gt;
&lt;/ol&gt;














&lt;figure  id=&#34;figure-json-file&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;JSON file&#34; srcset=&#34;
               /post/boa-tutorial/json_looklike_hu2ab2130319f6cfb527ad0ef2986ae93a_168440_f49bec1fdd27f7b5c178995e13009bd9.png 400w,
               /post/boa-tutorial/json_looklike_hu2ab2130319f6cfb527ad0ef2986ae93a_168440_20af3c33d6658053b2194ed22c8b5fb8.png 760w,
               /post/boa-tutorial/json_looklike_hu2ab2130319f6cfb527ad0ef2986ae93a_168440_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/post/boa-tutorial/json_looklike_hu2ab2130319f6cfb527ad0ef2986ae93a_168440_f49bec1fdd27f7b5c178995e13009bd9.png&#34;
               width=&#34;600&#34;
               height=&#34;395&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      JSON file
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;!-- &lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;img/json_looklike.png&#34; title=&#34;JSON file&#34;&gt;
&lt;/p&gt; --&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;In this way, for each project, that will be included in the dataset, create a JSON file. So, if you want to create a dataset for 5 projects, you will create 5 separate JSON files. Save all the JSON files in a folder. Alternatively, you can create a single JSON file for all of the projects by separating them by comma in an array ‘[]’. The format looks like [{}, {}]. Each curly brace is for one project.&lt;/li&gt;
&lt;li&gt;In Eclipse, go to the project compiler &amp;gt; src/java &amp;gt; boa.datagen, right click on BoaGenerator.java &amp;gt; Run As &amp;gt; Run Configurations.&lt;/li&gt;
&lt;li&gt;Double click on Java Application to create a new configuration. Browse project and select compiler, Search Main Class and select boa.datagen.BoaGenerator.&lt;/li&gt;
&lt;/ol&gt;














&lt;figure  id=&#34;figure-data-generator-configuration&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Data generator configuration&#34; srcset=&#34;
               /post/boa-tutorial/datagen_hu949fdfc0778c85be9ad23ef2b524f8af_286100_975fb2b00653598ae3d0aaaf66cd565c.png 400w,
               /post/boa-tutorial/datagen_hu949fdfc0778c85be9ad23ef2b524f8af_286100_a821359fabc7942f9039a41f15b7635b.png 760w,
               /post/boa-tutorial/datagen_hu949fdfc0778c85be9ad23ef2b524f8af_286100_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/post/boa-tutorial/datagen_hu949fdfc0778c85be9ad23ef2b524f8af_286100_975fb2b00653598ae3d0aaaf66cd565c.png&#34;
               width=&#34;718&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Data generator configuration
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;Select Arguments tab and add program arguments. The program arguments format should look like:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;  -inputJson 	&amp;lt;directory containing project JSON files&amp;gt;
  -output 	&amp;lt;output directory of the dataset (folder will be automatically created)&amp;gt;
  -inputRepo 	&amp;lt;temporary directory used to clone the projects (this folder will be automatically created)&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The other arguments are optional. For example, to print debug messages in console use -debug.&lt;/p&gt;

 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;figure  id=&#34;figure-data-generator-parameters&#34;&gt;
   &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
     &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Data generator parameters&#34; srcset=&#34;
                /post/boa-tutorial/datagenparam_hu0ab8dbc86191e859d9cf2dbaf834ceed_190851_9bcc7799a517c20c420a928e34056f90.png 400w,
                /post/boa-tutorial/datagenparam_hu0ab8dbc86191e859d9cf2dbaf834ceed_190851_f15015d26c48b89e9bc0530707a356ba.png 760w,
                /post/boa-tutorial/datagenparam_hu0ab8dbc86191e859d9cf2dbaf834ceed_190851_1200x1200_fit_lanczos_3.png 1200w&#34;
                src=&#34;https://sumonbis.github.io/post/boa-tutorial/datagenparam_hu0ab8dbc86191e859d9cf2dbaf834ceed_190851_9bcc7799a517c20c420a928e34056f90.png&#34;
                width=&#34;760&#34;
                height=&#34;650&#34;
                loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
   &lt;/div&gt;&lt;figcaption&gt;
       Data generator parameters
     &lt;/figcaption&gt;&lt;/figure&gt;
&lt;ol start=&#34;9&#34;&gt;
&lt;li&gt;Hit Run.&lt;/li&gt;
&lt;li&gt;This should start cloning the projects form Github and generating dataset. Depending on the number of projects and size of the projects, this will take some time to finish. When the red Terminate option in the console goes off, the data generation process is finished.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;run-boa-query-on-new-dataset&#34;&gt;Run Boa Query on New Dataset&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Create a dataset folder copying three files (projects.seq, ast/data, ast/index) from the generated output folder from step 8 of data generation process.&lt;/li&gt;
&lt;li&gt;In Eclipse, go to the project compiler &amp;gt; src/java &amp;gt; boa.evaluator, right click on BoaEvaluator.java &amp;gt; Run As &amp;gt; Run Configurations…&lt;/li&gt;
&lt;li&gt;Create a new configuration by clicking the New Configuration in the upper left corner of the window.&lt;/li&gt;
&lt;li&gt;Give a Name to the configuration, Browse project and select compiler, Search Main Class and select boa.evaluator.BoaEvaluator.&lt;/li&gt;
&lt;li&gt;Select Arguments tab and add program arguments. The program arguments format should look like:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;-input &amp;lt;file path to the boa source code file&amp;gt;
-data &amp;lt;dataset directory containing three files(projects.seq, data, index)&amp;gt;
-output &amp;lt;output directory&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;figure  id=&#34;figure-data-evaluator-parameters&#34;&gt;
   &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
     &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Data evaluator parameters&#34; srcset=&#34;
                /post/boa-tutorial/evalparam_hue8c49e7c5cd92ffdb7d8309808005f8d_259498_bd1849e8859a8e0de8f2553ea8b570e1.png 400w,
                /post/boa-tutorial/evalparam_hue8c49e7c5cd92ffdb7d8309808005f8d_259498_38c40ac3b74c73d956959061788cdf42.png 760w,
                /post/boa-tutorial/evalparam_hue8c49e7c5cd92ffdb7d8309808005f8d_259498_1200x1200_fit_lanczos_3.png 1200w&#34;
                src=&#34;https://sumonbis.github.io/post/boa-tutorial/evalparam_hue8c49e7c5cd92ffdb7d8309808005f8d_259498_bd1849e8859a8e0de8f2553ea8b570e1.png&#34;
                width=&#34;760&#34;
                height=&#34;467&#34;
                loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
   &lt;/div&gt;&lt;figcaption&gt;
       Data evaluator parameters
     &lt;/figcaption&gt;&lt;/figure&gt;
 &lt;!-- &lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;img/evalparam.png&#34; title=&#34;Data evaluator parameters&#34;&gt;
&lt;/p&gt; --&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;The output of the query will be printed in the console.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The screencast to go over the above steps and setup Boa development environment is shown in the following video.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/vuHUx-NYrOo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Computational Thinking of K-12 Outreach Program</title>
      <link>https://sumonbis.github.io/post/k-12-coding-competition-judge/</link>
      <pubDate>Sat, 14 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/post/k-12-coding-competition-judge/</guid>
      <description>&lt;p&gt;As part of the K-12 Outreach program, ISU computer science organized Computational Thinking Event this year. I got the opportunity to be a judge of the competition and see the outstanding computational projects developed by bright youngsters!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Science fair-like format where participants come prepared
to present their projects to a panel of experts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Computational thinking is critical skill that is needed for the success in the 21st century. The exposure to problem solving skills at an early stage helps the students greatly in their future endeavor. Since Fall 2010, the ISU Computer Science Department has been making efforts in Iowa K-12 schools with a series of events and workshops designed to help K-12 educators, parents, and students learn about computational thinking.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Trophies, Prizes, Awards of Excellence, and Certificates of
Participation. Prizes included laptops, tablets and Raspberry PIs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This particular event was last Saturday at Iowa State campus. The computer faculties who were involved in the outreach program are: Wallapak Tavanapong, Soma Chaudhuri, Yan-Bin Jia, Lu Ruan, Simanta Mitra, and Oliver Eulenstein.&lt;/p&gt;
&lt;p&gt;I joined the panel of judges along with other graduate students. I was really amazed to see how interesting and skillful projects were developed by the young students. The students competed in the following groups:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;K 3&lt;/li&gt;
&lt;li&gt;K 4-6&lt;/li&gt;
&lt;li&gt;K 7-9&lt;/li&gt;
&lt;li&gt;K 10-12&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Contestants could work in small groups of two or by themselves. The judgements were mainly based on the difficulty of the problem, cleverness of the solution, and ability of the students to explain.&lt;/p&gt;
&lt;p&gt;Most of the younger students were using the &lt;a href=&#34;https://scratch.mit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scratch platform developed in MIT&lt;/a&gt;, to learn and build interesting games. Scratch is designed and maintained by the Lifelong Kindergarten group at the MIT Media Lab. Here is an example project &lt;a href=&#34;https://scratch.mit.edu/projects/211107373/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Speech Helper 2.0&lt;/a&gt; built by Carson and McClain Crigger, both sixth graders at South Middle School in Waukee, IA. The program is designed to help their younger brother practice letter sounds.&lt;/p&gt;
&lt;iframe src=&#34;https://scratch.mit.edu/projects/211107373/embed&#34; allowtransparency=&#34;true&#34; width=&#34;485&#34; height=&#34;402&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;blockquote&gt;
&lt;p&gt;Scratch is a programming language and an online community where children can program and share interactive media such as stories, games, and animation with people from all over the world.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It was fascinating to see the coding skill of the kids and how they are engaged in the activity. There were three projects in the K 10-12 group which were solving complex problem with exciting ideas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Project &lt;strong&gt;Mardiop&lt;/strong&gt; by Adam&lt;/li&gt;
&lt;li&gt;Project &lt;strong&gt;Electric Field Simulation&lt;/strong&gt; by Brandt&lt;/li&gt;
&lt;li&gt;Project &lt;strong&gt;Fast Intersection&lt;/strong&gt; by Prithvi&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Those projects were rich in formulating the problem, organizing and analyzing data, and testing and managing their code. It was a great day to talk to them and share ideas.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
