[{"authors":null,"categories":null,"content":"I am a Ph.D. candidate at Department of Computer Science in Iowa State University. I am also working as a Research Assistant at Laboratory of Software Design. My supervisor is Dr. Hridesh Rajan. For my dissertation, I am working on improving fairness of machine learning models.\nResearch My research interests are in the intersection of software engineering (SE), data science (DS), and programming languages (PL). I have worked on mining open source repositories and conduct large-scale program analysis using the Boa framework. Especially, I worked on building Python language support for the Boa compiler and analyzing machine learning (ML) programs. I am interested in empirical research using rich source of metadata and code available in GitHub, StackOverflow, Kaggle, and solve SE problems such as bug detection, modularity, program evolution, code comprehension, etc.\nCurrently, I am working in the D4 (Dependable Data-Driven Discovery) project and focusing on increasing the dependability of data science software. Recently, my research highlighted unfairness in real-world ML models where I investigated the societal bias and their mitigation algorithms in data-driven systems. I am always open to interesting discussions and collaborations.\n  Download my resumé.\n","date":1621563933,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1621563933,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Ph.D. candidate at Department of Computer Science in Iowa State University. I am also working as a Research Assistant at Laboratory of Software Design. My supervisor is Dr.","tags":null,"title":"Sumon Biswas","type":"authors"},{"authors":null,"categories":null,"content":"Software fairness has been violated in many critical predictive applications in recent times. We have seen a number of those news in last few yers. The machine learning (ML) models used to make the predictions can exhibit bias for various reasons. In this project, we address the algorithmic fairness of the models, which is measured from the predictions of the model.\n  Reported fairness violations  Many research looked at the problem and proposed different measures and mitigations to make the models fairer. However, the prior works consider the ML model wholistically as a black-box, and do not look at the fairness of components in the ML pipeline. ML pipeline can have several components and stages such as preprocessing, training, tuning, evaluation, etc. Each of them can affect the ultimate fairness of the model. Our goal is to investigate the fairness in the component-level and identify the modules that are causing the unfairness.\n  Reported fairness violations  First, we do not consider the whole ML model as a single black box. Along with commenting on the fair of unfair behavior of the whole model, we look inside the black box and try to understand which components are responsible for the unfairness of the model. Our FSE'21 paper paper focused on identifying unfair preprocessing stages in ML pipeline.\n What is the fairness measure of a certain component/stage (e.g., imputation, scaling, etc.) in ML pipeline?   Look at the following ML pipeline which is taken from the crime prediction analysis repository of Propublica. The pipeline operates on Compas dataset that contains records of about 7k defendants in Florida. This was used at US courts in at least 10 states including New York, Wisconsin, California, Florida, etc 1. The pipeline transforms data using six data transformers before applying the LogisticRegression model. For example, in line 2-5, custom data filtration was applied, and in line 12, an imputation method from the library was applied to replace the missing values for the feature is_recid. When we measure the fairness of this model using existing metrics such as statistical parity or equal opportunity, that does not say anything about the fairness of these data transformers.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  df = pd.read_csv(f_path) df = df[(df.days_b_screening_arrest \u0026lt;= 30) \u0026amp; (df.days_b_screening_arrest \u0026gt;= -30) \u0026amp; (df.is_recid != -1) \u0026amp; (df.c_charge_degree != \u0026#39;O\u0026#39;) \u0026amp; (df.score_text != \u0026#39;N/A\u0026#39;)] df = df.replace(\u0026#39;Medium\u0026#39;, \u0026#39;Low\u0026#39;) labels = LabelEncoder().fit_transform(df.score_text) impute1_onehot = Pipeline([ (\u0026#39;imputer1\u0026#39;, SimpleImputer(strategy=\u0026#39;most_frequent\u0026#39;)), (\u0026#39;onehot\u0026#39;, OneHotEncoder(handle_unknown=\u0026#39;ignore\u0026#39;))]) impute2_bin = Pipeline([ (\u0026#39;imputer2\u0026#39;, SimpleImputer(strategy=\u0026#39;mean\u0026#39;)), (\u0026#39;discretizer\u0026#39;, KBinsDiscretizer(n_bins=4, encode=\u0026#39;ordinal\u0026#39;, strategy=\u0026#39;uniform\u0026#39;))]) featurizer = ColumnTransformer(transformers=[ (\u0026#39;impute1_onehot\u0026#39;, impute1_onehot, [\u0026#39;is_recid\u0026#39;]), (\u0026#39;impute2_bin\u0026#39;, impute2_bin, [\u0026#39;age\u0026#39;])]) pipeline = Pipeline([(\u0026#39;features\u0026#39;, featurizer), (\u0026#39;classifier\u0026#39;, LogisticRegression())])   We used causal reasoning in software to identify the fairness impact of those stages in the prediction.\nCausality in Software Identifying causal effects has been an integral part of scientific inquiry. It helped to answer a wide range of questions like - understanding behavior in online systems, or effect of social policies, or risk factors for diseases and so on.\nIn causal testing, given a failing test, causal experiments are conducted to find a set of test-passing inputs that are close to the failing input. In this project, we also used this casual modeling on the pipeline. We intervene on one variable of interest at a time and observe the change in the outcome.\n For two random variables $X$ and $Y$, we say that $X$ causes $Y$ when there exist at least two different interventions on $X$ that result in two different probability distributions of $Y$.   Causality in Fairness Causality in fairness has also been studied in the literature. “Other things being equal”, prediction would not have changed in the counterfactual world, where only the intervened variable would have changed.\nA predictor $\\hat{Y}$ is said to satisfy causal fairness if\n$$ P(\\hat{Y}(a, U) = y | X = x, A = a) = P(\\hat{Y}(a', U) = y | X = x, A = a) $$\nWe create an alternative pipeline $\\mathcal{P}* $ from the given pipeline $\\mathcal{P} $ by removing the preprocessing stage in consideration. Then we look at the prediction disparity between $\\mathcal{P} $ and $\\mathcal{P}* $. The disparity can be fairness satisfying or not. To evaluate that, we used the existing fairness criteria from the literature.\n Thus, we proposed four novel metrics to measure fairness of a stage in the ML pipeline.   We observed a number of patterns of fairness of the the data transformers that are commonly used in pipelines.\n  Data filtering and missing value removal change the data distribution and hence introduce bias in ML pipeline.\n  New feature generation or feature transformation can have large impact on fairness.\n  Encoding techniques should be chosen cautiosly based on the classifier.\n  Similar to the tradeoff between the accuracy and fairness for the classifier, the stages of the pipelines also exhibit the tradeoff. Often the accuracy-improve data transformer is unfair.\n  Among all the transformers, applying sampling technique exhibits most unfairness.\n  Selecting a subset of features often increase unfairness.\n  Feature standardization and non-linear transformers are fair transformers.\n  Furthermore, another impact that our method could attain is that we can instrument the pipeline. A pipeline can have a unfair stage that favors the privileged. Similarly, there can be another stage that favors the unprivileged. Both stages can be used in a pipeline such that their unfairness is canceled.\n The fairness composition can help to choose appropriate alternatives while development and improve the overall fairness.   We noticed that the most popular fairness packages (e.g., AIF360) uses a default data preprocessing each time users import datasets from the packages. There is no option to control or measure the unfair stages in the pipeline. Our early results would provide guidance to analyze fairness at a component-level. Further research in the area is in progress to understand fairness composition and optimize the pipeline construction.\n Sumon Biswas, Hridesh Rajan.  Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline. In 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE), 2021. Cite  Code  DOI   PDF   arXiv    --   https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1622160000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622160000,"objectID":"534bc5f72e3d9085729e1f3b1f923837","permalink":"https://sumonbis.github.io/project/modular-fairness/","publishdate":"2021-05-28T00:00:00Z","relpermalink":"/project/modular-fairness/","section":"project","summary":"We used causal reasoning to measure fairness of components and remove them from machine learning pipeline.","tags":["Fairness"],"title":"Component-level Fairness in Machine Learning Pipeline","type":"project"},{"authors":["Sumon Biswas","Hridesh Rajan"],"categories":["conference"],"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ","date":1621563933,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621563933,"objectID":"75a3bd006e2342f9c8ff200f1fce4304","permalink":"https://sumonbis.github.io/publication/esec-fse21/","publishdate":"2021-06-05T21:25:33-05:00","relpermalink":"/publication/esec-fse21/","section":"publication","summary":"We introduced the causal method of fairness to reason about the fairness impact of data preprocessing stages in ML pipeline. We leveraged existing metrics to define the fairness measures of the stages. Then we conducted a detailed fairness evaluation of the preprocessing stages in 37 pipelines collected from three different sources.","tags":["fairness","machine learning","preprocessing","pipeline","models"],"title":"Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline","type":"publication"},{"authors":["Sumon Biswas"],"categories":["Research"],"content":"The government, academia, industry are increasingly employing artificial intelligence (AI) systems in decision making. With the availability of numerous data, AI systems are becoming more popular in various sectors. Many of these systems affect human lives directly in one way or another. Our research highlighted that many of such real-world machine learning (ML) models exhibit unfairness with respect to certain societal groups of race, sex or age. In the last few years, our software design lab employed significant effort to identify fairness in machine learning algorithms and mitigate that effectively. Recent result shows that several components in an ML pipeline are influencing the predictive result that is unfair to minority groups such as dark-skinned people or female.\nI and my advisor Hridesh Rajan are working in the D4 Institute at Iowa State which broadly focuses on increasing the dependability of AI-based systems.\n The accuracy of a model is not always telling the whole story. How much bias the model propagates or how much we can trust the prediction is a big question.\n These AI-based software are being used in hiring employees, approving loans, criminal sentencing, which should be more accountable and explainable. Analyzing the behavior of ML pipeline in component level would help towards that goal. Our paper proposing a novel method to identify unfairness of ML components has been recently accepted in ESEC/FSE 2021, which is one of top software engineering conference and internationally renowned forum for researchers, practitioners, and educators.\nIn the paper, we proposed a causal method to measure the fairness of different stages in ML pipeline.\n Although some recent work proposed metrics to quantify the bias in the predictions, ML software is complex having several stages. Our method could effectively identify the data transformers that caused unfairness in such software.\n We strongly believe that the researchers and practitioners would be able to leverage our approach to avoid biased data preprocessing. Our goal in the long-run would be to unveil the ML black box and reason about fairness constraints.\nI will present the results of the paper entitled “Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline”, in the research track of the 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) to be held in Athens, Greece from August 23-28, 2021. The preprint of the paper is available here: https://arxiv.org/pdf/2106.06054.pdf.\nWe are excited on the acceptance of the paper in a top venue in our area. We are continuing the research to explore new avenues and assure the fairness of machine learning software.\n","date":1619913600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619913600,"objectID":"1ab4ba38aa03ed52dfdbc98a7c808bd7","permalink":"https://sumonbis.github.io/post/fair-pipeline/","publishdate":"2021-05-02T00:00:00Z","relpermalink":"/post/fair-pipeline/","section":"post","summary":"We proposed causal reasoning in machine learning pipeline to measure fairness of data preprocessing stages.","tags":["Fairness","Causal reasoning","Machine learning"],"title":"Our Research Identifies Unfairness in the Component Level of AI Based Software","type":"post"},{"authors":["Sumon Biswas","Hridesh Rajan"],"categories":["conference"],"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ","date":1604888733,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604888733,"objectID":"043ad64d98f6c6350150e48aa5466b16","permalink":"https://sumonbis.github.io/publication/esec-fse20/","publishdate":"2020-06-20T21:25:33-05:00","relpermalink":"/publication/esec-fse20/","section":"publication","summary":"We have focused on the empirical evaluation of fairness and mitigations on real-world machine learning models. We have created a benchmark of 40 top-rated models from Kaggle used for 5 different tasks, and then using a comprehensive set of fairness metrics, evaluated their fairness. Then, we have applied 7 mitigation techniques on these models and analyzed the fairness, mitigation results, and impacts on performance.","tags":["fairness","machine learning","models"],"title":"Do the Machine Learning Models on a Crowd Sourced Platform Exhibit Bias? An Empirical Study on Model Fairness","type":"publication"},{"authors":["Sumon Biswas"],"categories":["Tutorial"],"content":"YouTube Studio can be used to generate closed captions for your talk by following these simple steps:\n  First, record the video of the talk. The video should have a clear voice recording.\n  Sign in to YouTube Studio.\n  Click on CREATE \u0026gt; Upload videos. Then upload the recorded talk and fill the standard options (title, description, etc.). Finish the upload and publish it. It will take some time to upload and process the video.\n    Upload video on YouTube Studio  --  To be able to auto-generate the captions, select the Video language (e.g., English), and Caption certification (e.g., This content has never aired on television in the U.S.).     Steps for generating captions in YouTube Studio  After the video is published, select Videos from the left menu and click on the video. It will open the video editing options.    Video editing options  Click Subtitles from the left menu. You should see English(Automatic) subtitle option there. Click DUPLICATE AND EDIT.   This option might not be available immediately after uploading a video. For me, YouTube Studio took around half an hour to make the subtitle available for a 15-min video.     Generate subtitles  You should see the automatically generated text from the speech. You have two options to finalize the captions:   You can edit the text to make corrections: adding punctuations, capitalization, misspelled words, etc. The text will be automatically synced with the timing. However, you can modify the timings too. Note that, in one timeframe, you should not put a lot of characters which will cover much space on the screen. Alternatively, you can delete the automatically generated text and add your text manually by clicking on +CAPTION. Then write the text, start time, and end time.    Edit subtitles   Save the draft and click PUBLISH.\n  Finally, click on Options beside the EDIT button and click Download. Then select .srt format. Thus, you get your closed caption for your talk in a .srt file.\n   The .srt file is editable with a text editor as well. Be careful with the formatting and finally check whether the captions are working on a video player on your computer.   Here is an example talk with closed captions. Click on the CC button on the video if the closed captions are not enabled on YouTube. You can get other details about YouTube Studio captioning here.\nOther Options Apart from YouTube Studio, there are other tools too for generating captions (.srt).\n  Maestra is an online service to generate and export subtitles. You can sign in, upload your video talk, transcribe, edit the auto-generated subtitles, and then export .srt.\n  Otter.ai can be used to do the task. You can find the instructions here. Although its a paid service, there could be some free trials.\n  Rev is another paid service to generate captions manually by human experts.\n  Other tools can be also used. Depending on the backend transcribe engine, the correctness can vary. However, you should edit the auto-generated text to finalize the closed captions.\n","date":1602547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605225600,"objectID":"577a1b5e81872c2d5d845fe5f2d00d71","permalink":"https://sumonbis.github.io/post/closed-captions/","publishdate":"2020-10-13T00:00:00Z","relpermalink":"/post/closed-captions/","section":"post","summary":"This tutorial shows how to generate closed captions for your talk using YouTube Studio.","tags":["Talk","Closed-captions"],"title":"How to Add Closed Captions in Your Video Talk","type":"post"},{"authors":["Sumon Biswas"],"categories":["Research"],"content":"D4 Institute is an interdisciplinary data science hub at Iowa State university where professors, graduate students, REU students, and researchers from Computer Science, Electrical Engineering, Mathematics, Statistics collaborate to ensure the dependability of data science.\nD4 institute took about four year to assemble and then funded by NSF TRIPOD grant in 2020. My advisor Hridesh Rajan leads the project as an PI. I have been involved with D4 from the beginning of writing the NSF proposal. Afterwards, I continue to contribute as a graduate researcher in the project.\nThe computer science magazine Atanasoff Today featured out work in the D4 Institute. The magazine is available in here: https://www.cs.iastate.edu/atanasoff-today-piecing-together-premier-data-science-research-hub\n Sumon Biswas (’21 computer science, Ph.D.) was immediately drawn to Iowa State’s computer science program, in part because of Rajan’s group. Research opportunities related to the data science field matched nicely with his career goals. Biswas was particularly drawn to the group’s commitment to researching the data science pipeline.\n The central goal of the project is to ensure the dependability of data-driven software. With the growing interests in AI and machine learning, we need to focus on the safety, security, fairness, robustness, and more critical properties of such systems.\n “My research interests are very specific and tailored. My career focus blends software engineering, programming languages and data science,” Biswas said. “The varied research opportunities at Iowa State, in particular with the D4 Institute, allowed me to become an entrepreneur of sorts and design my own career that fit with my goals.”\n My advisor guided through the process to delve into the area and make original contribution in the project. We are continuing to work in the area and blend the software engineering and programming language expertise to bring more reliability on the AI and ML based systems.\n Rajan has provided Biswas with a rich array of opportunities that have shaped his career path. In addition to engaging in cutting-edge research on the data science life cycle, Biswas provided significant contributions to the development of the successful TRIPODS NSF grant. He also attended the Midwest Big Data Summer School where he learned cutting-edge research methods that further drew him into studying the data science life cycle.\n Specifically, I looked deep into the data science pipeline, which is a ordered set of stages including data collection, exploratory analysis, data preprocessing, modeling, training, evaluation, and different properties of the pipeline.\n “It’s been incredible,” Biswas said. “I’ve learned novel research ideas from D4 researchers and practitioners who have introduced me to studying the data science pipeline and its properties.”\n I already published my research work on ensuring fairness of machine learning models. The work analyzes different fairness measures, mitigation techniques, and their impacts in real-world ML based software.\n Biswas is close to publishing his own research which he conducted at the D4 Institute. “It’s exciting to be involved in research that could improve software systems, which affect many people who are impacted by data-driven decisions,” he said.\n  Rajan and his team plan to hire additional undergraduates, graduate students and postdocs at the D4 Institute. More students, like Biswas, will benefit from the experience of conducting NSF-funded research and working with seasoned experts who collaborate on studies.\n We have a full-grown team of collaborators now, undergraduate and graduate students, postdocs, industry partners, and professors from different expertise. I have also mentored undergraduate students and collaborated with others, which was a great experience to gain further knowledge, and share thoughts and ideas.\n","date":1597449600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600387200,"objectID":"74442bf515b9a9a635ce128d77164161","permalink":"https://sumonbis.github.io/post/d4-institute/","publishdate":"2020-08-15T00:00:00Z","relpermalink":"/post/d4-institute/","section":"post","summary":"D4 Institute is an interdisciplinary data science hub at Iowa State university where professors, graduate students, REU students, and researchers from Computer Science, Electrical Engineering, Mathematics, Statistics collaborate to ensure the dependability of data science.","tags":["Data science","Machine learning","Interdisciplinary"],"title":"Being a Part of A Premier Data Science Research Hub","type":"post"},{"authors":null,"categories":null,"content":"Software engineering (SE) research has focused on identifying, localizing, and repairing defects in software systems. With the evolution of programming languages, coding practices and requirements, various kinds of bugs of software has drawn interest. Recent trend of machine learning techniques has raised the question of dependability of the predictive decisions. Fairness of the machine learning (ML) models has been an open issue for such software systems in last few years. In critical decision making such as loan approval, crime prediction, college admission or hiring employees, ML algorithms are being used. However, the problem of fairness has not been studied by the SE community as much as the ML community looked into it. We conducted an empirical study in our FSE'20 paper to investigate a number of SE concerns of measures, mitigation, and impact of unfairness in ML models.\n It is important to ensure the fairness of the ML models so that no discrimination is made based on protected attribute (e.g., race, sex, age) while decision making.\n First, it is important to look at the real-world software systems to understand the state of unfairness. Most of the study experiment using default-set classifiers from libraries. Thus, often the real faults and concerns with fairness are missed. However, it might be difficult to find such open-source models in the wild. We found a good number of models from Kaggle that helped to point down fairness issues in the wild.\nFairness Measures The foremost big concern is that how do we know there is a fairness issue in my model. Well, it depends on the stakeholder and the context. The participants could ask for equal opportunity, while the owners focus on the disparate impact. A popular library for fairness, AIF360 has more than 70 metrics to measure fairness. Two most common used broad categories of fairness measures are:\n Group fairness Individual fairness  The impossibility theorem of fairness suggests that satisfying fairness with respect to all the definition can not be achievable, since one definition can contradict another. Furthermore, fairness in classification problem has been focused is most of the works, while fairness of regression or ranking problems is not yet well studied.\n Choosing appropriate fairness metrics and optimizing them simultaneously is an important requirement engineering problem to be solved.   We found several interesting finding from our experiments:\n  Only one fairness metric would not show the whole picture. There is no metric that combines multiples metric too.\n  Fairness metrics have their assumptions as well. Most of the group fairness metrics require the dataset divided into privileged and non-privileged. So, it can not compute unfairness among multiple groups (e.g., Asian, American, African, etc.) at the same time. Furthermore, the metrics measure fairness with respect to one protected attribute (race or sex or age). A fair model with respect to sex could be biased with respect to age.\n  Developers often tend to optimize the models for accuracy, which causes unfairness.\n  The library APIs such as classifiers from Scikit-Learn have several constructs that affect fairness. But those are not document and developers are aware of their fairness impacts.\n  It is often thought that if data is fair, the model would be so too. Although data has a great deal of impact of fairness of the decisions, a model could be fairer which used biased data. At the same time, a model could be biased which was trained on unbiased data. We found that some data preparation techniques, thus, can have both positive or negative impact on fairness. Specifically, the data standardization and feature engineering (removal or creation) techniques affect fairness almost always.\n  Bias Mitigation There are many bias mitigation techniques proposed in the literature, which are grouped into these categories:\n Preprocessing techniques: These techniques operate on the training data to make the resulting model fairer. In-processing techniques: These techniques alters the existing mode. Post-processing techniques: The predictions are changes with some constraints to make them fairer.   Choosing a bias mitigation technique can be also confusing. Applying one mitigation technique can work for a particular metric and hamper accuracy as well as another fairness metric.   In our paper, we reported several observations that help to choose a mitigation technique for the model.\n  Effective preprocessing mitigation technique is preferable, since it does not ruin accuracy in most of the cases.\n  If the model exhibits a lot of bias, post-processing techniques are the most successful.\n  Originally fairer models are debiased effectively by preprocessing or in-processing techniques.\n  Impacts Fairnes does not come free. In most of models, there is a accuracy loss when the mitigation technique are applied. Multi-objective optimization could be utilized to improve fairness and accuracy together.\n  In-processing mitigation algorithms show uncertain behavior in their performance.\n  Although post-processing algorithms are the most dominating in debiasing, they are always diminishing the model accuracy and F1 score.\n  Trade-off between performance and fairness exists.\n  Overall, ensuring fairness require to address many dimensions of the problem at the same time. There is a wrong perception that only debiasing training data would suffice to make the model fair. The training is not always uniform and it does not learn from all the data instances equally. So, its needed to make the learning process better as well. Because of the tradeoff, multiple metrics, domain-specific issues, we need further testing and verification mechanisms as well. A few studies proposed fairness aware language constructs and test input generation techniques. However, we need further languages, tools, and methods to localize the fairness issue and repair them in real-world situations.\n","date":1589846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589846400,"objectID":"4fe69a435339d8181a0f74598e2d38b6","permalink":"https://sumonbis.github.io/project/empirical-fairness/","publishdate":"2020-05-19T00:00:00Z","relpermalink":"/project/empirical-fairness/","section":"project","summary":"We have studied the software engineering concerns of fairness in real-world machine learning models.","tags":["Fairness"],"title":"SE Concerns of Fairness in ML Models","type":"project"},{"authors":null,"categories":null,"content":"Data science (DS) is everywhere now. The chart below shows the increasing number of publications with the topic “machine-learning” in the title. Also, the number of open source data science repositories in GitHub is growing very rapidly. Mining Software Repository have been very successful in recent times for SE research. Some datasets like Dacapo, Quallitas created new opportunity for MSR research. However, there is no dataset available to analyze DS software written in Python language. So, we created this dataset by mining open-source repositories from GitHub. The dataset was published in MSR 2019.\n  Mined data from GitHub DS repositories   We created a dataset that contains top rated 1,558 DS projects from Github that are written in Python. For storing and analyzing efficiently, we have stored the dataset in Hadoop sequence file. The dataset is available in Boa platform.   The details about the parsing, mining, and usage can be found here.   Dataset Details Different metrics of the dataset in showed in the table at the top. We used several filtering criteria to select top-rated data DS repositories. The properties of the dataset are:\n  Filtering criteria to select GitHub repos.   Original (not forked) project with Python as the primary language. Contains at least one data science keywords like machine-learning, deep neural network in the description of the project. The whole list of keywords are as follows:  \u0026quot;machine learn\u0026quot;, \u0026quot;machine-learn\u0026quot;, \u0026quot;data sci\u0026quot;, \u0026quot;data-sci\u0026quot;, \u0026quot;big data\u0026quot;, \u0026quot;big-data\u0026quot;, \u0026quot;large data\u0026quot;, \u0026quot;large-data\u0026quot;, \u0026quot;data analy\u0026quot;, \u0026quot;data-analy\u0026quot;, \u0026quot;deep learn\u0026quot;, \u0026quot;deep-learn\u0026quot;, \u0026quot;data model\u0026quot;, \u0026quot;data-model\u0026quot;, \u0026quot;artificial intel\u0026quot;, \u0026quot;artificial-intel\u0026quot;, \u0026quot;mining\u0026quot;, \u0026quot;topic modelling\u0026quot;, \u0026quot;topic-modelling\u0026quot;, \u0026quot;natural language pro\u0026quot;, \u0026quot;natural-language-pro\u0026quot;, \u0026quot;nlp\u0026quot;, \u0026quot;data frame\u0026quot;, \u0026quot;data proces\u0026quot;, \u0026quot; ml \u0026quot;, \u0026quot;tensorflow\u0026quot;, \u0026quot;tensor flow\u0026quot;, \u0026quot;tensor-flow\u0026quot;, \u0026quot;theano\u0026quot;, \u0026quot;caffe\u0026quot;, \u0026quot;keras\u0026quot;, \u0026quot;scikit-learn\u0026quot;, \u0026quot;kaggle\u0026quot;, \u0026quot;spark\u0026quot;, \u0026quot;hadoop\u0026quot;, \u0026quot;mapreduce\u0026quot;, \u0026quot;hdfs\u0026quot;, \u0026quot;neural net\u0026quot;, \u0026quot;neural-net\u0026quot;  Contains at least one usage of data science library like Pytorch, Caffe, Keras, Tensorflow, etc. A full list of used 33 Python data science libraries are listed below:  \u0026quot;theano\u0026quot;, \u0026quot;pytroch\u0026quot;, \u0026quot;caffe\u0026quot;, \u0026quot;keras\u0026quot;, \u0026quot;tensorflow\u0026quot;, \u0026quot;sklearn\u0026quot;, \u0026quot;numpy\u0026quot;, \u0026quot;scipy\u0026quot;, \u0026quot;pandas\u0026quot;, \u0026quot;statsmodels\u0026quot;, \u0026quot;matplotlib\u0026quot;, \u0026quot;seaborn\u0026quot;, \u0026quot;plotly\u0026quot;, \u0026quot;bokeh\u0026quot;, \u0026quot;pydot\u0026quot;, \u0026quot;xgboost\u0026quot;, \u0026quot;catboost\u0026quot;, \u0026quot;lightgbm\u0026quot;, \u0026quot;eli5\u0026quot;, \u0026quot;elephas\u0026quot;, \u0026quot;spark\u0026quot;, \u0026quot;nltk\u0026quot;, \u0026quot;cntk\u0026quot;, \u0026quot;scrapy\u0026quot;, \u0026quot;gensim\u0026quot;, \u0026quot;pybrain\u0026quot;, \u0026quot;lightning\u0026quot;, \u0026quot;spacy\u0026quot;, \u0026quot;pylearn2\u0026quot;, \u0026quot;nupic\u0026quot;, \u0026quot;pattern\u0026quot;, \u0026quot;imblearn\u0026quot;, \u0026quot;pyenv\u0026quot;  Each repository contains at least 80 star.  The dataset contains projects owned by both organizations and individual users. Some of the top rated projects are Tensorflow Models, Keras, Scikit-learn, Pandas, Spacy, Spotify Luigi, NVIDIA FastPhotoStyle, Theano, etc.\n A full list of all the 1,558 Github projects are available here.    350 projects in the dataset are maintained by different organizations (Google, Microsoft, NVIDIA etc.). The list of organizations is here. The rest 1,208 projects are maintained by individual users. The list of users is here.  Availability The dataset is available in Boa infrastructure. Go to the Boa web interface and login. If you do not have an account, you can request a user.\nThen click on the Run Examples menu and select 2020 August/Python-DS in the input dataset dropdown option. Finally, you can paste the Boa code and mine desired information.\n The code, dataset details, and sample Boa queries are shared in this repository.   Usage To use the dataset go to Boa website and follow the steps:\n From the left menu, select User Login to login as a registered user. If you are not registered, request for a user. Write a query under the Boa Source Code. If researchers are not familiar with the language, the example Boa programs can be utilized by clicking the Select Examples. Some good examples for this dataset can be also found from the Github repository. Select 2020 August/Python-DS dataset in the drop-down list under Input Dataset and run the query.  The job will be submitted to Hadoop cluster and is executed parallely on the dataset. When the job status is finished, the output text file will be available for downloading. The job is saved for future reference. One can share the job with others and one can reproduce the result.\nTo learn about Boa language and queries, navigate through the Boa website, especially Programming Guide Section.\n Update: We have improved the framework to parse and mine Jupyter Notebooks along with the Python files.   ","date":1569110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569110400,"objectID":"34236cd837e0441e0358114120b4d8c8","permalink":"https://sumonbis.github.io/project/github-dataset/","publishdate":"2019-09-22T00:00:00Z","relpermalink":"/project/github-dataset/","section":"project","summary":"This dataset is created by mining 5M Python program snapshots. The code is transformed to AST for static analysis.","tags":["BigCode","SE4ML"],"title":"ML Repo Dataset from GitHub","type":"project"},{"authors":["Sumon Biswas","Md Johirul Islam","Yijia Huang","Hridesh Rajan"],"categories":["conference"],"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ","date":1558923933,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558923933,"objectID":"cc177aef3c74e96982510ef93b0d86c7","permalink":"https://sumonbis.github.io/publication/msr19/","publishdate":"2019-04-20T21:25:33-05:00","relpermalink":"/publication/msr19/","section":"publication","summary":"The popularity of Python programming language has surged in recent years due to its increasing usage in Data Science. The availability of Python repositories in Github presents an opportunity for mining software repository research, e.g., suggesting the best practices in developing Data Science applications, identifying bug-patterns, recommending code enhancements, etc. To enable this research, we have created a new dataset that includes 1,558 mature Github projects that develop Python software for Data Science tasks.","tags":["msr","boa","ast","machine learning","data science","oss","program analysis"],"title":"Boa Meets Python: A Boa Dataset of Data Science Software in Python Language","type":"publication"},{"authors":null,"categories":null,"content":"The usage of data science (DS) techniques have increase immensely in the recent past. With the development of artificial intelligence (AI) and machine learning (ML) techniques, and availability of huge amount of data led to the rapid growth of DS components in software. To enable further software engineering (SE) research of DS software in the wild, we built an infrastructure to mine and analyze DS software in large-scale. Our goal is to:\n Learn from the past and guide future development of DS software Improve software design and reuse Manage DS software better Automatic bug detection and repair Many more \u0026hellip;  We think that SE for these special kind of software system is necessary. To that end, we leveraged the huge amount of code and metadata available in open-source GitHub repositories. We proposed a methodology of filtering high-quality DS repositories and then mine code of all the revisions and project metadata e.g., number of developers, commit logs, creation-date, etc.\n  Mined data from GitHub DS repositories  We used the existing Boa framework to mine from GitHub. Python has been used as a de-facto of ML based software for a long time now. However, Boa could not parse and mine Python code. Therefore, I built the infrastructure to parse the Python code, transform that into AST, and store in Boa\u0026rsquo;s Protobuf format for storing. I used ANTLR grammar for Python 2 and Python 3 for parsing the source. Finally, the dataset is stored in the Hadoop cluster for further analysis. You can learn about how to use the Boa for mining new datasets from this tutorial. The first version of the dataset contained about 5 million Python files (including all revisions) from top rated 1558 DS repositories.\n  The first version of the mined dataset  The rich amount of data facilitates analyzing several research questions:\nData science development practices: Due to the recent ‘boom’ in machine learning software development, a lot of questions regearing DS best practices can be asked:\n What are the most used data preprocessing library? Why data dimension bugs are more frequent in deep neural networks? What data visualization APIs are most used in last two years?  Software change: Since the dataset contains all the snapshots of Python files, how developers change source can be studied.\nProgram analysis: The differences between traditional and DS software development can be studied. For example, do the code complexity or API usages follow similar pattern in DS software?\nSE practices: Several SE research can be conducted on the DS projects in the area of:\n DS bug detection and repair Code comprehension Testing DS modules  Project metadata: The project metadata can be leveraged to answer questions such as, what are the coding style differences between projects developed by individuals and organizations, etc.\nFurther details about the dataset can be found here. The domain-specific Boa language helps to perform program analysis query the metadata easily and quickly. For example, the following code snippet quickly count the number of bug-fixing revisions for all the projects in the dataset.\n# Counting the number of bug-fixing revisons for all projects p: Project = input; counts: output sum[string] of int; visit(p, visitor { before n: Project -\u0026gt; ; before node: CodeRepository -\u0026gt; ; before node: Revision -\u0026gt; if (isfixingrevision(node.log)) counts[p.name] \u0026lt;\u0026lt; 1; });  If we want to go deeper and mine the all the method calls in the bug-fixing revisions, we can use the following Boa program.\n Example Boa program for mining method calls in the bug fixing commits.   Note that the output contains 74.11 million results. We use commit logs to identify whether its a bug-fixing revision or not and then visit the expression to get the method calls. Boa uses automatic parallelization to run the query in Haddop cluster. Users do not need to worry about the storage and low-level details.\nAs of this writing, no open source dataset for studying Data Science software is available. We created a infrastructure and dataset to mins DS projects from GitHub that are using Python.\n","date":1555372800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555372800,"objectID":"a1efbc7e1b1ce32f1a863309a9471aef","permalink":"https://sumonbis.github.io/project/mining-ml/","publishdate":"2019-04-16T00:00:00Z","relpermalink":"/project/mining-ml/","section":"project","summary":"Mining and analyzing data-science repositories can provide development guidance and insights from historical data.","tags":["BigCode","SE4ML"],"title":"Large-Scale Mining of Data-Science Software from GitHub","type":"project"},{"authors":["Sumon Biswas"],"categories":["Tutorial"],"content":"This tutorial will describe how to create your own Boa dataset for specific projects in Github and run Boa queries on that dataset locally. We will use command line and Eclipse IDE for that purpose.\nPrerequisite You need to have following already installed in your system:\n JDK Apache Ant Git Eclipse IDE  Development Setup Steps  Clone the Boa project using the command line: git clone https://github.com/boalang/compiler.git Go to the cloned repository: cd compiler Clean the project: ant clean Create a directory for libraries: mkdir -p build/classes Compile the project: ant compile Create a class folder: mkdir compile In Eclipse go to: File \u0026gt; Open Projects from File System \u0026gt; Import Source – Directory \u0026gt; Browse the cloned repository (compiler) \u0026gt; Hit Finish    Import compiler project in Eclipse  -- Right click on the project compiler \u0026gt; Build Path \u0026gt; Configure Build Path In Source tab, click Add Folder to add the required source folders and remove unnecessary folder(s). After adding all the folders, the window should look like the following:    After configuring Eclipse  -- Select Libraries tab in the same window and click on Add Class Folder and add the compiler/compile folder that has been created in step 6. Click Add JARs… in the same Libraries tab \u0026gt; select lib \u0026gt; select all the files inside lib, including files under datagen and evaluator folder \u0026gt; hit Apply and Close. From the compiler project in Eclipse, right click on build.xml \u0026gt; Run as \u0026gt; 1 Ant build. This should build the project successfully. The development setup is completed. Now, we will move on to data generation steps.  Boa Data Generation Steps  Go to github.com and search the project for which you want to create the dataset. For example, if you want to create dataset for Apache Mavan project, go to https://github.com/apache/maven . Invoke a GitHub http-based RESTful API to get the metadata of the project by constructing a URL https://api.github.com/repos/repo_full_name, for example https://api.github.com/repos/apache/maven. Copy the whole JSON metadata, create a blank text file, type a pair of brackets ‘[]’, paste the metadata inside the brackets. Search for \u0026ldquo;languages_url\u0026rdquo; field in the JSON data, go to the URL, copy everything, create another field in the JSON file (\u0026ldquo;language_list\u0026rdquo;: ), paste copied text and save the file as filename.json (e.g., maven.json). The last few lines of the JSON file should look like:    JSON file  -- In this way, for each project, that will be included in the dataset, create a JSON file. So, if you want to create a dataset for 5 projects, you will create 5 separate JSON files. Save all the JSON files in a folder. Alternatively, you can create a single JSON file for all of the projects by separating them by comma in an array ‘[]’. The format looks like [{}, {}]. Each curly brace is for one project. In Eclipse, go to the project compiler \u0026gt; src/java \u0026gt; boa.datagen, right click on BoaGenerator.java \u0026gt; Run As \u0026gt; Run Configurations. Double click on Java Application to create a new configuration. Browse project and select compiler, Search Main Class and select boa.datagen.BoaGenerator.    Data generator configuration  Select Arguments tab and add program arguments. The program arguments format should look like:   -inputJson \u0026lt;directory containing project JSON files\u0026gt; -output \u0026lt;output directory of the dataset (folder will be automatically created)\u0026gt; -inputRepo \u0026lt;temporary directory used to clone the projects (this folder will be automatically created)\u0026gt;  The other arguments are optional. For example, to print debug messages in console use -debug.\n  Data generator parameters  Hit Run. This should start cloning the projects form Github and generating dataset. Depending on the number of projects and size of the projects, this will take some time to finish. When the red Terminate option in the console goes off, the data generation process is finished.  Run Boa Query on New Dataset  Create a dataset folder copying three files (projects.seq, ast/data, ast/index) from the generated output folder from step 8 of data generation process. In Eclipse, go to the project compiler \u0026gt; src/java \u0026gt; boa.evaluator, right click on BoaEvaluator.java \u0026gt; Run As \u0026gt; Run Configurations… Create a new configuration by clicking the New Configuration in the upper left corner of the window. Give a Name to the configuration, Browse project and select compiler, Search Main Class and select boa.evaluator.BoaEvaluator. Select Arguments tab and add program arguments. The program arguments format should look like:  -input \u0026lt;file path to the boa source code file\u0026gt; -data \u0026lt;dataset directory containing three files(projects.seq, data, index)\u0026gt; -output \u0026lt;output directory\u0026gt;    Data evaluator parameters  -- The output of the query will be printed in the console.  ","date":1531440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558828800,"objectID":"874fc4ed6a8e6a441982bc12c4d7ab26","permalink":"https://sumonbis.github.io/post/boa-tutorial/","publishdate":"2018-07-13T00:00:00Z","relpermalink":"/post/boa-tutorial/","section":"post","summary":"This tutorial demonstrates how to setup Boa on local machine and generate new GitHub dataset.","tags":["MSR","Static Analysis","Mining"],"title":"Boa for Big-Code Mining and Large-Scale Static Analysis","type":"post"},{"authors":["Manan B T Noor","Sumon Biswas"],"categories":["conference"],"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ","date":1432261533,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1432261533,"objectID":"891c6a41b22c84b9e97cfb7072703953","permalink":"https://sumonbis.github.io/publication/iceeict15b/","publishdate":"2015-05-21T21:25:33-05:00","relpermalink":"/publication/iceeict15b/","section":"publication","summary":"This paper shows a concern on the security element in cloud environment for small business addressing their shortcomings and finding solutions for it. Measured security features have been implemented by developing a secured data encryption, exchange and decryption infrastructure resulting in a data security model.","tags":["security","cloud computing","data science"],"title":"A Secure Data Security Infrastructure for Small Organization in Cloud Computing","type":"publication"},{"authors":["Sumon Biswas","M Shamim Kaiser","Shamin Al Mamun"],"categories":["conference"],"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ","date":1432261533,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1432261533,"objectID":"29621d9548271a58b442022e18b171a8","permalink":"https://sumonbis.github.io/publication/iceeict15a/","publishdate":"2015-05-21T21:25:33-05:00","relpermalink":"/publication/iceeict15a/","section":"publication","summary":"Ant colony optimization (ACO) based algorithm has been proposed which will generate set of optimal paths and prioritize the paths. Additionally, the approach generates test data sequence within the domain to use as inputs of the generated paths. Proposed approach guarantees full software coverage with minimum redundancy.","tags":["aco","genetic","testing","software","program analysis"],"title":"Applying Ant Colony Optimization in Software testing to Generate Prioritized Optimal Path and Test Data","type":"publication"},{"authors":["Sumon Biswas","Anisuzzaman","Tanjina Akhter","M Shamim Kaiser","Shamim Al Mamun"],"categories":["conference"],"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   ","date":1419301533,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1419301533,"objectID":"36020c0ae04d945ff716655927736813","permalink":"https://sumonbis.github.io/publication/iccit14/","publishdate":"2014-12-22T21:25:33-05:00","relpermalink":"/publication/iccit14/","section":"publication","summary":"In this article, a three tier cloud based application “eHealth Cloud” has been developed which will involve different parties to improve old-fashioned healthcare system. RIA (Rich Internet Application) based client, SimpleDB based server and a logic layer have been designed to build an easily accessible network.","tags":["cloud computing","data science","healthcare"],"title":"Cloud Based Healthcare Application Architecture and Electronic Medical Record Mining: An Integrated Approach to Improve Healthcare System","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://sumonbis.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]