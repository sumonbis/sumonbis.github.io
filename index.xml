<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sumon Biswas</title>
    <link>https://sumonbis.github.io/</link>
      <atom:link href="https://sumonbis.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Sumon Biswas</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2021 Sumon Biswas</copyright><lastBuildDate>Sun, 14 May 2023 21:25:33 -0500</lastBuildDate>
    <image>
      <url>https://sumonbis.github.io/media/icon_hub4cc8572db2cf4087d387aa00fd8abd9_555_512x512_fill_lanczos_center_3.png</url>
      <title>Sumon Biswas</title>
      <link>https://sumonbis.github.io/</link>
    </image>
    
    <item>
      <title>Fairify: Fairness Verification of Neural Networks</title>
      <link>https://sumonbis.github.io/publication/icse23a/</link>
      <pubDate>Sun, 14 May 2023 21:25:33 -0500</pubDate>
      <guid>https://sumonbis.github.io/publication/icse23a/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Towards Understanding Fairness and its Composition in Ensemble Machine Learning</title>
      <link>https://sumonbis.github.io/publication/icse23b/</link>
      <pubDate>Sun, 14 May 2023 21:20:33 -0500</pubDate>
      <guid>https://sumonbis.github.io/publication/icse23b/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>SPLASH 2021 Retrospective: Organizing a Premier PL Conference in Hybrid</title>
      <link>https://sumonbis.github.io/post/splash/</link>
      <pubDate>Sat, 06 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/post/splash/</guid>
      <description>&lt;p&gt;I served as an Accessibility Chair in the organizing committee of the &lt;a href=&#34;https://2020.splashcon.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPLASH conference in 2020&lt;/a&gt;.
SPLASH is the premier conference on the applications of programming languages- at the intersection of programming languages and software engineering. It hosted the &lt;strong&gt;OOPSLA, ECOOP, and other co-located events&lt;/strong&gt;. Because of the pandemic, SPLASH held in 2020 into a virtual setting.
I facilitated the audio-video (AV) accessibility support for the authors and audiences. Especially, I enabled closed-captions for all the talks in the conference for their talks for better accessibility. I provided instructions and tutorials to all the authors for creation of closed captions of their talks. Despite several challenges of correction and synchronization, I was able to provide accessibility support to the conference.&lt;/p&gt;
&lt;p&gt;The success of virtualization of SPLASH 2020 was recognized and appreciated by the organizers, and in the following year I served as the Accessibility Chair in SPLASH 2021 as well. This time &lt;a href=&#34;https://2021.splashcon.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPLASH 2021&lt;/a&gt; was hybrid with close to three hundred in-person participation in Chicago and over four hundred virtual attendees. I observed a great positivity and inclusive culture in the conference which was possible through an easy-to-access medium of operation and technical support. The hybrid conference also showed the benefits of inclusivity in a great deal. It showed that even after pandemic, if we have a hybrid setting in such conferences, a lot more people can join and interact internationally, which is often very limited from many underrepresented country. For example, a student from India could not have traveled to Chicago because of several barriers like visa, travel-cost, etc. However, these people are a big part of the community in terms of publications and representation. The hybrid event gives almost a seamless experience for their participation. Albeit a lot to improve for the hybridization, we wrote a SIGPLAN blog on the hybrid conference organization in 2021.&lt;/p&gt;
&lt;p&gt;Me along with Jonathan Aldrich, Steve Blackburn, Benjamin Chung, Youyou Cong, Alex Potanin, Hridesh Rajan, and Talia Ringer, shared our experience in this &lt;a href=&#34;https://blog.sigplan.org/2022/08/25/hybrid-splash-2021-retrospective/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SIGPLAN blog&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>23 Shades of Self-Admitted Technical Debt: An Empirical Study on Machine Learning Software</title>
      <link>https://sumonbis.github.io/publication/esec-fse22/</link>
      <pubDate>Tue, 14 Jun 2022 21:25:33 -0500</pubDate>
      <guid>https://sumonbis.github.io/publication/esec-fse22/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>The Art and Practice of Data Science Pipelines: A Comprehensive Study of Data Science Pipelines In Theory, In-The-Small, and In-The-Large</title>
      <link>https://sumonbis.github.io/publication/icse22/</link>
      <pubDate>Sat, 21 May 2022 21:25:33 -0500</pubDate>
      <guid>https://sumonbis.github.io/publication/icse22/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Design and Architecture of Data Science Pipelines</title>
      <link>https://sumonbis.github.io/project/design-architecture-ds/</link>
      <pubDate>Sat, 18 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/project/design-architecture-ds/</guid>
      <description>&lt;p&gt;Data science processes are becoming an integral component of many software systems today. In data-driven software, the processes are organized in several stages such as data acquisition, data preprocessing, modeling, training, evaluation, prediction, and so on, where the data flow from one stage to another. The stages with different subtasks, their connections, and feedback loops, create a new kind of software architecture called Data Science Pipeline. In order to design and build software systems with data science stages effectively, we must understand the structure of the data science pipelines. We demonstrated the importance of standardization and analysis framework for data science pipelines. We took the first step to understand the architecture and patterns of data science pipelines from theory and practice. &lt;/p&gt;
&lt;p&gt;We conducted a three-pronged study to draw observations from pipelines in the literature and popular press, smaller data science tasks, and large projects. They investigated the representation of the pipeline structure, its organization, and characteristics. What are the typical stages of a data science pipeline? How are they connected? Do the pipelines differ in the theoretical representations and that in the practice? Today we do not fully understand these architectural characteristics. The study resulted in three representative pipeline structures. The work also informs the terminology and design criteria for pipelines. For example, a number of stages from theory are absent in the pipelines in small data science programs without a clear separation of stages. On the other hand, the pipelines in large data science projects develop complex pipelines with feedback loops and sub-pipelines. The stage boundaries are stricter in large projects, which is necessary for scalability, maintenance, and testing of pipelines. The results will facilitate pipeline architects, practitioners, and software engineering teams to compare with existing and representative pipelines. For instance, a data scientist can identify whether the pipeline is missing any important stage or feedback loops in an earlier stage of development lifecycle, which will save much time and effort.&lt;/p&gt;
&lt;p&gt;We presented the results of the paper entitled &amp;ldquo;The Art and Practice of Data Science Pipelines: A Comprehensive Study of Data Science Pipelines In Theory, In-The-Small, and In-The-Large&amp;rdquo;, in the research technical track of the 44th International Conference on Software Engineering (ICSE 2022) held in Pittsburgh, PA, USA from May 21-29, 2022.&lt;/p&gt;
&lt;p&gt;The project has been supported in part by the National Science Foundation TRIPODS institute at ISU called D4 (Dependable Data-Driven Discovery). D4 has a broader goal of increasing dependability in data science pipelines by addressing various critical properties of pipelines such as fairness, complexity, uncertainty, and so on. The preprint of the paper is now available. Also, the artifact containing data and code are evaluated and published for future work in the area.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Causal Fairness in Machine Learning Pipeline</title>
      <link>https://sumonbis.github.io/project/modular-fairness/</link>
      <pubDate>Fri, 28 May 2021 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/project/modular-fairness/</guid>
      <description>&lt;p&gt;Software fairness has been violated in many critical predictive applications in recent times. We have seen a number of those news in last few yers.
The machine learning (ML) models used to make the predictions can exhibit bias for various reasons. In this project, we address the &lt;em&gt;algorithmic fairness&lt;/em&gt; of the models, which is measured from the predictions of the model.&lt;/p&gt;














&lt;figure  id=&#34;figure-reported-fairness-violations&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Reported fairness violations&#34; srcset=&#34;
               /project/modular-fairness/news_hu3233d3db2e418be631d6910f666245d9_5012327_46019d95c58d1b31ccdfb7e1814ec458.jpg 400w,
               /project/modular-fairness/news_hu3233d3db2e418be631d6910f666245d9_5012327_2a36cd5627ba236ea243abbddeb4c04f.jpg 760w,
               /project/modular-fairness/news_hu3233d3db2e418be631d6910f666245d9_5012327_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://sumonbis.github.io/project/modular-fairness/news_hu3233d3db2e418be631d6910f666245d9_5012327_46019d95c58d1b31ccdfb7e1814ec458.jpg&#34;
               width=&#34;760&#34;
               height=&#34;390&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Reported fairness violations
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Many research looked at the problem and proposed different measures and mitigations to make the models fairer. However, the prior works consider the ML model wholistically as a black-box, and do not look at the fairness of components in the ML pipeline. ML pipeline can have several components and stages such as preprocessing, training, tuning, evaluation, etc. Each of them can affect the ultimate fairness of the model. Our goal is to investigate the fairness in the component-level and identify the modules that are causing the unfairness.&lt;/p&gt;














&lt;figure  id=&#34;figure-reported-fairness-violations&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Reported fairness violations&#34; srcset=&#34;
               /project/modular-fairness/black-box_hu309927eb576099c7084cf9aed61d13b6_83467_c2d7ec41a2a83694864295033989e2f5.png 400w,
               /project/modular-fairness/black-box_hu309927eb576099c7084cf9aed61d13b6_83467_586a4e5f594e218262325243c671596d.png 760w,
               /project/modular-fairness/black-box_hu309927eb576099c7084cf9aed61d13b6_83467_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/project/modular-fairness/black-box_hu309927eb576099c7084cf9aed61d13b6_83467_c2d7ec41a2a83694864295033989e2f5.png&#34;
               width=&#34;432&#34;
               height=&#34;196&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Reported fairness violations
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;First, we do not consider the whole ML model as a single black box. Along with commenting on the fair of unfair behavior of the whole model, we look inside the black box and try to understand which components are responsible for the unfairness of the model. &lt;a href=&#34;https://sumonbis.github.io/publication/esec-fse21/&#34;&gt;Our FSE&#39;21 paper&lt;/a&gt; paper focused on identifying unfair preprocessing stages in ML pipeline.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    What is the fairness measure of a certain component/stage (e.g., imputation, scaling, etc.) in ML pipeline?
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Look at the following ML pipeline which is taken from the &lt;a href=&#34;https://github.com/propublica/compas-analysis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;crime prediction analysis repository of Propublica&lt;/a&gt;. The pipeline operates on Compas dataset that contains records of about 7k defendants in Florida. This was used at US courts in at least 10 states including New York, Wisconsin, California, Florida, etc &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. The pipeline transforms data using six data transformers before applying the LogisticRegression model. For example, in line 2-5, custom data filtration was applied, and in line 12, an imputation method from the library was applied to replace the missing values for the feature &lt;code&gt;is_recid&lt;/code&gt;. When we measure the fairness of this model using existing metrics such as statistical parity or equal opportunity, that does not say anything about the fairness of these data transformers.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;
&lt;table style=&#34;border-spacing:0;padding:0;margin:0;border:0;&#34;&gt;&lt;tr&gt;&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 1
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 2
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 3
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 4
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 5
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 6
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 7
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 8
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 9
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;10
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;11
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;12
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;13
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;14
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;15
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;16
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;17
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;18
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;;width:100%&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(f_path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[(df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;days_b_screening_arrest &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; (df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;days_b_screening_arrest &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; (df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;is_recid &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; (df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;c_charge_degree &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;O&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; (df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;score_text &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;N/A&amp;#39;&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;replace(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Medium&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Low&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; LabelEncoder()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_transform(df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;score_text)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;impute1_onehot &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Pipeline([
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;imputer1&amp;#39;&lt;/span&gt;, SimpleImputer(strategy&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;most_frequent&amp;#39;&lt;/span&gt;)),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;onehot&amp;#39;&lt;/span&gt;, OneHotEncoder(handle_unknown&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ignore&amp;#39;&lt;/span&gt;))])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;impute2_bin &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Pipeline([
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;imputer2&amp;#39;&lt;/span&gt;, SimpleImputer(strategy&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;)),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;discretizer&amp;#39;&lt;/span&gt;, KBinsDiscretizer(n_bins&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, encode&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ordinal&amp;#39;&lt;/span&gt;, strategy&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;uniform&amp;#39;&lt;/span&gt;))])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;featurizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ColumnTransformer(transformers&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;impute1_onehot&amp;#39;&lt;/span&gt;, impute1_onehot, [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;is_recid&amp;#39;&lt;/span&gt;]),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;impute2_bin&amp;#39;&lt;/span&gt;, impute2_bin, [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;age&amp;#39;&lt;/span&gt;])])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Pipeline([(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;features&amp;#39;&lt;/span&gt;, featurizer),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;classifier&amp;#39;&lt;/span&gt;, LogisticRegression())])&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We used causal reasoning in software to identify the fairness impact of those stages in the prediction.&lt;/p&gt;
&lt;h2 id=&#34;causality-in-software&#34;&gt;Causality in Software&lt;/h2&gt;
&lt;p&gt;Identifying causal effects has been an integral part of scientific inquiry. It helped to answer a wide range of questions like - understanding behavior in online systems, or effect of social policies, or risk factors for diseases and so on.&lt;/p&gt;
&lt;p&gt;In causal testing, given a failing test, causal experiments are conducted to find a set of test-passing inputs that are close to the failing input.
In this project, we also used this casual modeling on the pipeline. We intervene on one variable of interest at a time and observe the change in the outcome.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    For two random variables $X$ and $Y$, we say that $X$ causes $Y$ when there exist at least two different interventions on $X$ that result in two different probability distributions of $Y$.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;causality-in-fairness&#34;&gt;Causality in Fairness&lt;/h2&gt;
&lt;p&gt;Causality in fairness has also been studied in the literature. “Other things being equal”, prediction would not have changed in the counterfactual world, where only the intervened variable would have changed.&lt;/p&gt;
&lt;p&gt;A predictor $\hat{Y}$ is said to satisfy causal fairness if&lt;/p&gt;
&lt;p&gt;$$
P(\hat{Y}(a, U) = y | X = x, A = a) = P(\hat{Y}(a&amp;rsquo;, U) = y | X = x, A = a)
$$&lt;/p&gt;
&lt;p&gt;We create an alternative pipeline $\mathcal{P}* $ from the given pipeline $\mathcal{P} $ by removing the preprocessing stage in consideration. Then we look at the prediction disparity between $\mathcal{P} $ and $\mathcal{P}* $. The disparity can be fairness satisfying or not. To evaluate that, we used the existing fairness criteria from the literature.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Thus, we proposed four novel metrics to measure fairness of a stage in the ML pipeline.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We observed a number of patterns of fairness of the the data transformers that are commonly used in pipelines.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Data filtering and missing value removal change the data distribution and hence introduce bias in ML pipeline.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;New feature generation or feature transformation can have large impact on fairness.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Encoding techniques should be chosen cautiosly based on the classifier.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Similar to the tradeoff between the accuracy and fairness for the classifier, the stages of the pipelines also exhibit the tradeoff. Often the accuracy-improve data transformer is unfair.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Among all the transformers, applying sampling technique exhibits most unfairness.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Selecting a subset of features often increase unfairness.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Feature standardization and non-linear transformers are fair transformers.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Furthermore, another impact that our method could attain is that we can instrument the pipeline. A pipeline can have a unfair stage that favors the privileged. Similarly, there can be another stage that favors the unprivileged. Both stages can be used in a pipeline such that their unfairness is canceled.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    The fairness composition can help to choose appropriate alternatives while development and improve the overall fairness.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We noticed that the most popular fairness packages (e.g., &lt;a href=&#34;https://aif360.readthedocs.io/en/latest/#&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AIF360&lt;/a&gt;) uses a default data preprocessing each time users import datasets from the packages. There is no option to control or measure the unfair stages in the pipeline. Our early results would provide guidance to analyze fairness at a component-level. Further research in the area is in progress to understand fairness composition and optimize the pipeline construction.&lt;/p&gt;
&lt;!-- 






  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span class=&#34;author-highlighted&#34;&gt;
      Sumon Biswas&lt;/span&gt;, &lt;span &gt;
      Hridesh Rajan&lt;/span&gt;
  &lt;/span&gt;
  &lt;div&gt; 
    &lt;a href=&#34;https://sumonbis.github.io/publication/esec-fse21/&#34;&gt;Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline&lt;/a&gt;
  &lt;/div&gt;
  In 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE), Athens, Greece,
  2021.

  
  &lt;div class=&#34;btn-links&#34;&gt;
    








  





&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/esec-fse21/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/sumonbis/FairPreprocessing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1145/3468264.3468536&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


  
  
  
    
  
  
  
  
  
    
  
  &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://design.cs.iastate.edu/papers/ESEC-FSE-21/fair-preprocessing-fse21.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
    &lt;i class=&#34;fas fa-file-pdf mr-1&#34;&gt;&lt;/i&gt;
    PDF
  &lt;/a&gt;

  
  
  
  
  
  
  
    
  
  &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/2106.06054&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
    &lt;i class=&#34;ai ai-arxiv mr-1&#34;&gt;&lt;/i&gt;
    arXiv
  &lt;/a&gt;

  
  
  
    
  
  
  
  
  
    
  
  &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://youtu.be/X-Nvn6DhHsA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
    &lt;i class=&#34;fab fa-youtube mr-1&#34;&gt;&lt;/i&gt;
    Talk
  &lt;/a&gt;


  &lt;/div&gt;
  

  
&lt;/div&gt;

  

 --&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline</title>
      <link>https://sumonbis.github.io/publication/esec-fse21/</link>
      <pubDate>Thu, 20 May 2021 21:25:33 -0500</pubDate>
      <guid>https://sumonbis.github.io/publication/esec-fse21/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Our Research Identifies Unfairness in the Component Level of AI Based Software</title>
      <link>https://sumonbis.github.io/post/fair-pipeline/</link>
      <pubDate>Sun, 02 May 2021 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/post/fair-pipeline/</guid>
      <description>&lt;p&gt;The government, academia, industry are increasingly employing artificial intelligence (AI) systems in decision making. With the availability of numerous data, AI systems are becoming more popular in various sectors. Many of these systems affect human lives directly in one way or another. Our research highlighted that many of such real-world machine learning (ML) models exhibit unfairness with respect to certain societal groups of race, sex or age. In the last few years, our software design lab employed significant effort to identify fairness in machine learning algorithms and mitigate that effectively. Recent result shows that several components in an ML pipeline are influencing the predictive result that is unfair to minority groups such as dark-skinned people or female.&lt;/p&gt;
&lt;p&gt;I and my advisor Hridesh Rajan are working in the D4 Institute at Iowa State which broadly focuses on increasing the dependability of AI-based systems.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The accuracy of a model is not always telling the whole story. How much bias the model propagates or how much we can trust the prediction is a big question.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;These AI-based software are being used in hiring employees, approving loans, criminal sentencing, which should be more accountable and explainable. Analyzing the behavior of ML pipeline in component level would help towards that goal.
&lt;a href=&#34;https://sumonbis.github.io/publication/esec-fse21/&#34;&gt;Our paper proposing a novel method to identify unfairness of ML components&lt;/a&gt; has been recently accepted in &lt;a href=&#34;https://2021.esec-fse.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ESEC/FSE 2021&lt;/a&gt;, which is one of top software engineering conference and internationally renowned forum for researchers, practitioners, and educators.&lt;/p&gt;
&lt;p&gt;In the paper, we proposed a causal method to measure the fairness of different stages in ML pipeline.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Although some recent work proposed metrics to quantify the bias in the predictions, ML software is complex having several stages. Our method could effectively identify the data transformers that caused unfairness in such software.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We strongly believe that the researchers and practitioners would be able to leverage our approach to avoid biased data preprocessing. Our goal in the long-run would be to unveil the ML black box and reason about fairness constraints.&lt;/p&gt;
&lt;p&gt;I will present the results of the paper entitled “Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline”, in the research track of the 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) to be held in Athens, Greece from August 23-28, 2021. The preprint of the paper is available here: &lt;a href=&#34;https://arxiv.org/pdf/2106.06054.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/2106.06054.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We are excited on the acceptance of the paper in a top venue in our area. We are continuing the research to explore new avenues and assure the fairness of machine learning software.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Do the Machine Learning Models on a Crowd Sourced Platform Exhibit Bias? An Empirical Study on Model Fairness</title>
      <link>https://sumonbis.github.io/publication/esec-fse20/</link>
      <pubDate>Sun, 08 Nov 2020 21:25:33 -0500</pubDate>
      <guid>https://sumonbis.github.io/publication/esec-fse20/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>How to Add Closed Captions in Your Video Talk</title>
      <link>https://sumonbis.github.io/post/closed-captions/</link>
      <pubDate>Tue, 13 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/post/closed-captions/</guid>
      <description>&lt;p&gt;YouTube Studio can be used to generate closed captions for your talk by following these simple steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;First, record the video of the talk. The video should have a clear voice recording.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sign in to &lt;a href=&#34;https://studio.youtube.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YouTube Studio&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click on &lt;code&gt;CREATE &amp;gt; Upload videos&lt;/code&gt;. Then upload the recorded talk and fill the standard options (title, description, etc.). Finish the upload and publish it. It will take some time to upload and process the video.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- 













&lt;figure  id=&#34;figure-upload-video-on-youtube-studio&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Upload video on YouTube Studio&#34; srcset=&#34;
               /post/closed-captions/upload_hu08fd69e9e84e7425239b423688b7ff4a_12001_14a869470a1b89444bd8053e0b9be2c5.png 400w,
               /post/closed-captions/upload_hu08fd69e9e84e7425239b423688b7ff4a_12001_bacc2bca902070aa18ce762b949e71d9.png 760w,
               /post/closed-captions/upload_hu08fd69e9e84e7425239b423688b7ff4a_12001_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/post/closed-captions/upload_hu08fd69e9e84e7425239b423688b7ff4a_12001_14a869470a1b89444bd8053e0b9be2c5.png&#34;
               width=&#34;331&#34;
               height=&#34;93&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Upload video on YouTube Studio
    &lt;/figcaption&gt;&lt;/figure&gt; --&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    To be able to auto-generate the captions, select the &lt;code&gt;Video language&lt;/code&gt; (e.g., English), and &lt;code&gt;Caption certification&lt;/code&gt; (e.g., This content has never aired on television in the U.S.).
  &lt;/div&gt;
&lt;/div&gt;














&lt;figure  id=&#34;figure-steps-for-generating-captions-in-youtube-studio&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Steps for generating captions in YouTube Studio&#34;
           src=&#34;https://sumonbis.github.io/post/closed-captions/process.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Steps for generating captions in YouTube Studio
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;After the video is published, select &lt;code&gt;Videos&lt;/code&gt; from the left menu and click on the video. It will open the video editing options.&lt;/li&gt;
&lt;/ol&gt;














&lt;figure  id=&#34;figure-video-editing-options&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Video editing options&#34; srcset=&#34;
               /post/closed-captions/editing_hudc065b6217f6da97f5e9c3d73465d578_22345_3d670f216c10cc93f023eaaf0526bb8b.png 400w,
               /post/closed-captions/editing_hudc065b6217f6da97f5e9c3d73465d578_22345_b48179162dd89ab02128900adb791046.png 760w,
               /post/closed-captions/editing_hudc065b6217f6da97f5e9c3d73465d578_22345_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/post/closed-captions/editing_hudc065b6217f6da97f5e9c3d73465d578_22345_3d670f216c10cc93f023eaaf0526bb8b.png&#34;
               width=&#34;760&#34;
               height=&#34;115&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Video editing options
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Click &lt;code&gt;Subtitles&lt;/code&gt; from the left menu. You should see &lt;code&gt;English(Automatic)&lt;/code&gt; subtitle option there. Click &lt;code&gt;DUPLICATE AND EDIT&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    This option might not be available immediately after uploading a video. For me, YouTube Studio took around half an hour to make the subtitle available for a 15-min video.
  &lt;/div&gt;
&lt;/div&gt;














&lt;figure  id=&#34;figure-generate-subtitles&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Generate subtitles&#34; srcset=&#34;
               /post/closed-captions/subtitles_hu497f034dd612990d1e15ac6f0579d693_42581_e90dbf9fa7cfe5ec320ad045a2288857.png 400w,
               /post/closed-captions/subtitles_hu497f034dd612990d1e15ac6f0579d693_42581_5af088a46693f0902883d5f0a5929749.png 760w,
               /post/closed-captions/subtitles_hu497f034dd612990d1e15ac6f0579d693_42581_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/post/closed-captions/subtitles_hu497f034dd612990d1e15ac6f0579d693_42581_e90dbf9fa7cfe5ec320ad045a2288857.png&#34;
               width=&#34;760&#34;
               height=&#34;212&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Generate subtitles
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;You should see the automatically generated text from the speech. You have two options to finalize the captions:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;You can edit the text to make corrections: adding punctuations, capitalization, misspelled words, etc. The text will be automatically synced with the timing. However, you can modify the timings too. Note that, in one timeframe, you should not put a lot of characters which will cover much space on the screen.&lt;/li&gt;
&lt;li&gt;Alternatively, you can delete the automatically generated text and add your text manually by clicking on &lt;code&gt;+CAPTION&lt;/code&gt;. Then write the text, start time, and end time.&lt;/li&gt;
&lt;/ul&gt;














&lt;figure  id=&#34;figure-edit-subtitles&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Edit subtitles&#34; srcset=&#34;
               /post/closed-captions/timing_hu8393fbf032e093c702c75fdff53cba05_64692_2221ea3bec630bb61bd1f5818125c1bf.png 400w,
               /post/closed-captions/timing_hu8393fbf032e093c702c75fdff53cba05_64692_1c789b521e0ff1e46e3d1ffaa14ec2cc.png 760w,
               /post/closed-captions/timing_hu8393fbf032e093c702c75fdff53cba05_64692_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/post/closed-captions/timing_hu8393fbf032e093c702c75fdff53cba05_64692_2221ea3bec630bb61bd1f5818125c1bf.png&#34;
               width=&#34;529&#34;
               height=&#34;308&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Edit subtitles
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;
&lt;p&gt;Save the draft and click &lt;code&gt;PUBLISH&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, click on &lt;code&gt;Options&lt;/code&gt; beside the &lt;code&gt;EDIT&lt;/code&gt; button and click &lt;code&gt;Download&lt;/code&gt;. Then select &lt;code&gt;.srt&lt;/code&gt; format. Thus, you get your closed caption for your talk in a &lt;code&gt;.srt&lt;/code&gt; file.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    The &lt;code&gt;.srt&lt;/code&gt; file is editable with a text editor as well. Be careful with the formatting and finally check whether the captions are working on a video player on your computer.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- https://support.google.com/youtube/answer/2734796?hl=en#zippy= --&gt;
&lt;p&gt;Here is an &lt;a href=&#34;https://youtu.be/C7lfPoMbpIA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;example talk&lt;/a&gt; with closed captions. Click on the CC button on the video if the closed captions are not enabled on YouTube. You can get other details about YouTube Studio captioning &lt;a href=&#34;https://support.google.com/youtube/answer/2734796?hl=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;other-options&#34;&gt;Other Options&lt;/h2&gt;
&lt;p&gt;Apart from YouTube Studio, there are other tools too for generating captions (&lt;code&gt;.srt&lt;/code&gt;).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://maestrasuite.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Maestra&lt;/a&gt; is an online service to generate and export subtitles. You can sign in, upload your video talk, transcribe, edit the auto-generated subtitles, and then export &lt;code&gt;.srt&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://otter.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Otter.ai&lt;/a&gt; can be used to do the task. You can find the instructions &lt;a href=&#34;https://blog.otter.ai/video-captions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. Although its a paid service, there could be some free trials.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.rev.com/caption&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rev&lt;/a&gt; is another paid service to generate captions manually by human experts.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Other tools can be also used. Depending on the backend transcribe engine, the correctness can vary. However, you should edit the auto-generated text to finalize the closed captions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Being a Part of A Premier Data Science Research Hub</title>
      <link>https://sumonbis.github.io/post/d4-institute/</link>
      <pubDate>Sat, 15 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/post/d4-institute/</guid>
      <description>&lt;p&gt;D4 Institute is an interdisciplinary data science hub at Iowa State university where professors, graduate students, REU students, and researchers from Computer Science, Electrical Engineering, Mathematics, Statistics collaborate to ensure the dependability of data science.&lt;/p&gt;
&lt;p&gt;D4 institute took about four year to assemble and then funded by NSF TRIPOD grant in 2020. My advisor Hridesh Rajan leads the project as an PI. I have been involved with D4 from the beginning of writing the NSF proposal. Afterwards, I continue to contribute as a graduate researcher in the project.&lt;/p&gt;
&lt;p&gt;The computer science magazine Atanasoff Today featured out work in the D4 Institute. The magazine is available in here: &lt;a href=&#34;https://www.cs.iastate.edu/atanasoff-today-piecing-together-premier-data-science-research-hub&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cs.iastate.edu/atanasoff-today-piecing-together-premier-data-science-research-hub&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Sumon Biswas (’21 computer science, Ph.D.) was immediately drawn to Iowa State’s computer science program, in part because of Rajan’s group. Research opportunities related to the data science field matched nicely with his career goals. Biswas was particularly drawn to the group’s commitment to researching the data science pipeline.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The central goal of the project is to ensure the dependability of data-driven software. With the growing interests in AI and machine learning, we need to focus on the safety, security, fairness, robustness, and more critical properties of such systems.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“My research interests are very specific and tailored. My career focus blends software engineering, programming languages and data science,” Biswas said. “The varied research opportunities at Iowa State, in particular with the D4 Institute, allowed me to become an entrepreneur of sorts and design my own career that fit with my goals.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;My advisor guided through the process to delve into the area and make original contribution in the project. We are continuing to work in the area and blend the software engineering and programming language expertise to bring more reliability on the AI and ML based systems.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Rajan has provided Biswas with a rich array of opportunities that have shaped his career path. In addition to engaging in cutting-edge research on the data science life cycle, Biswas provided significant contributions to the development of the successful TRIPODS NSF grant. He also attended the Midwest Big Data Summer School where he learned cutting-edge research methods that further drew him into studying the data science life cycle.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Specifically, I looked deep into the data science pipeline, which is a ordered set of stages including data collection, exploratory analysis, data preprocessing, modeling, training, evaluation, and different properties of the pipeline.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“It’s been incredible,” Biswas said. “I’ve learned novel research ideas from D4 researchers and practitioners who have introduced me to studying the data science pipeline and its properties.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I already published my research work on ensuring fairness of machine learning models. The work analyzes different fairness measures, mitigation techniques, and their impacts in real-world ML based software.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Biswas is close to publishing his own research which he conducted at the D4 Institute. “It’s exciting to be involved in research that could improve software systems, which affect many people who are impacted by data-driven decisions,” he said.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Rajan and his team plan to hire additional undergraduates, graduate students and postdocs at the D4 Institute. More students, like Biswas, will benefit from the experience of conducting NSF-funded research and working with seasoned experts who collaborate on studies.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We have a full-grown team of collaborators now, undergraduate and graduate students, postdocs, industry partners, and professors from different expertise. I have also mentored undergraduate students and collaborated with others, which was a great experience to gain further knowledge, and share thoughts and ideas.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fairness Engineering in ML Models</title>
      <link>https://sumonbis.github.io/project/empirical-fairness/</link>
      <pubDate>Tue, 19 May 2020 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/project/empirical-fairness/</guid>
      <description>&lt;p&gt;Software engineering (SE) research has focused on identifying, localizing, and repairing defects in software systems. With the evolution of programming languages, coding practices and requirements, various kinds of bugs of software has drawn interest. Recent trend of machine learning techniques has raised the question of dependability of the predictive decisions. Fairness of the machine learning (ML) models has been an open issue for such software systems in last few years. In critical decision making such as loan approval, crime prediction, college admission or hiring employees, ML algorithms are being used. However, the problem of fairness has not been studied by the SE community as much as the ML community looked into it. We conducted an empirical study in &lt;a href=&#34;https://sumonbis.github.io/publication/esec-fse20/&#34;&gt;our FSE&#39;20 paper&lt;/a&gt; to investigate a number of SE concerns of measures, mitigation, and impact of unfairness in ML models.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It is important to ensure the fairness of the ML models so that no discrimination is made based on protected attribute (e.g., race, sex, age) while decision making.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;First, it is important to look at the real-world software systems to understand the state of unfairness. Most of the study experiment using default-set classifiers from libraries. Thus, often the real faults and concerns with fairness are missed. However, it might be difficult to find such open-source models in the wild. We found a good number of models from Kaggle that helped to point down fairness issues in the wild.&lt;/p&gt;
&lt;h2 id=&#34;fairness-measures&#34;&gt;Fairness Measures&lt;/h2&gt;
&lt;p&gt;The foremost big concern is that how do we know there is a fairness issue in my model. Well, it depends on the stakeholder and the context. The participants could ask for &lt;em&gt;equal opportunity&lt;/em&gt;, while the owners focus on the &lt;em&gt;disparate impact&lt;/em&gt;. A popular library for fairness, &lt;a href=&#34;https://aif360.readthedocs.io/en/latest/#&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AIF360&lt;/a&gt; has more than 70 metrics to measure fairness. Two most common used broad categories of fairness measures are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Group fairness&lt;/li&gt;
&lt;li&gt;Individual fairness&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;em&gt;impossibility theorem&lt;/em&gt; of fairness suggests that satisfying fairness with respect to all the definition can not be achievable, since one definition can contradict another. Furthermore, fairness in classification problem has been focused is most of the works, while fairness of regression or ranking problems is not yet well studied.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Choosing appropriate fairness metrics and optimizing them simultaneously is an important requirement engineering problem to be solved.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We found several interesting finding from our experiments:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Only one fairness metric would not show the whole picture. There is no metric that combines multiples metric too.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fairness metrics have their assumptions as well. Most of the group fairness metrics require the dataset divided into privileged and non-privileged. So, it can not compute unfairness among multiple groups (e.g., Asian, American, African, etc.) at the same time. Furthermore, the metrics measure fairness with respect to one protected attribute (&lt;em&gt;race&lt;/em&gt; or &lt;em&gt;sex&lt;/em&gt; or &lt;em&gt;age&lt;/em&gt;). A fair model with respect to &lt;em&gt;sex&lt;/em&gt; could be biased with respect to &lt;em&gt;age&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Developers often tend to optimize the models for accuracy, which causes unfairness.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The library APIs such as classifiers from &lt;a href=&#34;https://scikit-learn.org/stable/modules/classes.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scikit-Learn&lt;/a&gt; have several constructs that affect fairness. But those are not document and developers are aware of their fairness impacts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is often thought that if data is fair, the model would be so too. &lt;mark&gt;Although data has a great deal of impact of fairness of the decisions, a model could be fairer which used biased data&lt;/mark&gt;. At the same time, a model could be biased which was trained on unbiased data. We found that some data preparation techniques, thus, can have both positive or negative impact on fairness. Specifically, the data standardization and feature engineering (removal or creation) techniques affect fairness almost always.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;bias-mitigation&#34;&gt;Bias Mitigation&lt;/h2&gt;
&lt;p&gt;There are many bias mitigation techniques proposed in the literature, which are grouped into these categories:&lt;/p&gt;
&lt;!-- {style=&#34;color: red&#34;} --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Preprocessing techniques:&lt;/strong&gt; These techniques operate on the training data to make the resulting model fairer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;In-processing techniques:&lt;/strong&gt; These techniques alters the existing mode.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Post-processing techniques:&lt;/strong&gt; The predictions are changes with some constraints to make them fairer.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Choosing a bias mitigation technique can be also confusing. Applying one mitigation technique can work for a particular metric and hamper accuracy as well as another fairness metric.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In our paper, we reported several observations that help to choose a mitigation technique for the model.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Effective preprocessing mitigation technique is preferable, since it does not ruin accuracy in most of the cases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the model exhibits a lot of bias, post-processing techniques are the most successful.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Originally fairer models are debiased effectively by preprocessing or in-processing
techniques.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;impacts&#34;&gt;Impacts&lt;/h2&gt;
&lt;p&gt;Fairnes does not come free. In most of models, there is a accuracy loss when the mitigation technique are applied. Multi-objective optimization could be utilized to improve fairness and accuracy together.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;In-processing mitigation algorithms show uncertain behavior in their performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Although post-processing algorithms are the most dominating in debiasing, they are always diminishing the model accuracy and F1 score.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Trade-off between performance and fairness exists.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Overall, ensuring fairness require to address many dimensions of the problem at the same time. There is a wrong perception that only debiasing training data would suffice to make the model fair. The training is not always uniform and it does not learn from all the data instances equally. So, its needed to make the learning process better as well. Because of the tradeoff, multiple metrics, domain-specific issues, we need further testing and verification mechanisms as well. A few studies proposed fairness aware language constructs and test input generation techniques. However, we need further languages, tools, and methods to localize the fairness issue and repair them in real-world situations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ML Repo Dataset from GitHub</title>
      <link>https://sumonbis.github.io/project/github-dataset/</link>
      <pubDate>Sun, 22 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/project/github-dataset/</guid>
      <description>&lt;p&gt;Data science (DS) is everywhere now. The chart below shows the increasing number of publications with the topic “machine-learning” in the title. Also, the number of open source data science repositories in GitHub is growing very rapidly. Mining Software Repository have been very successful in recent times for SE research. Some datasets like &lt;em&gt;Dacapo&lt;/em&gt;, &lt;em&gt;Quallitas&lt;/em&gt; created new opportunity for MSR research. However, there is no dataset available to analyze DS software written in Python language. So, we created this dataset by mining open-source repositories from GitHub. The dataset was published in &lt;a href=&#34;https://sumonbis.github.io/publication/msr19/&#34;&gt;MSR 2019&lt;/a&gt;.&lt;/p&gt;














&lt;figure  id=&#34;figure-mined-data-from-github-ds-repositories&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Mined data from GitHub DS repositories&#34; srcset=&#34;
               /project/github-dataset/trend_hu2957a9f13c4dfc74f41ecccf55c83cd5_27281_453272fbc0e11653a2c50f7a3b49605f.png 400w,
               /project/github-dataset/trend_hu2957a9f13c4dfc74f41ecccf55c83cd5_27281_395ac01bc8541054539bb097da390d0c.png 760w,
               /project/github-dataset/trend_hu2957a9f13c4dfc74f41ecccf55c83cd5_27281_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/project/github-dataset/trend_hu2957a9f13c4dfc74f41ecccf55c83cd5_27281_453272fbc0e11653a2c50f7a3b49605f.png&#34;
               width=&#34;432&#34;
               height=&#34;248&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Mined data from GitHub DS repositories
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;ol&gt;
&lt;li&gt;We created a dataset that contains top rated 1,558 DS projects from Github that are written in Python.&lt;/li&gt;
&lt;li&gt;For storing and analyzing efficiently, we have stored the dataset in Hadoop sequence file.&lt;/li&gt;
&lt;li&gt;The dataset is available in &lt;a href=&#34;http://boa.cs.iastate.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boa&lt;/a&gt; platform.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    The details about the parsing, mining, and usage can be found &lt;a href=&#34;https://sumonbis.github.io/project/mining-ml/&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;dataset-details&#34;&gt;Dataset Details&lt;/h2&gt;
&lt;p&gt;Different metrics of the dataset in showed in the table at the top. We used several filtering criteria to select top-rated data DS repositories. The properties of the dataset are:&lt;/p&gt;














&lt;figure  id=&#34;figure-filtering-criteria-to-select-github-repos&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Filtering criteria to select GitHub repos.&#34; srcset=&#34;
               /project/github-dataset/filtering_hu3fdd9194a17a6c0c3dc5b94305164096_101605_3d4b5db43492ea1e3e3151c7a34470a1.png 400w,
               /project/github-dataset/filtering_hu3fdd9194a17a6c0c3dc5b94305164096_101605_de2c44f83f085cb50c0c5a7f7b799442.png 760w,
               /project/github-dataset/filtering_hu3fdd9194a17a6c0c3dc5b94305164096_101605_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/project/github-dataset/filtering_hu3fdd9194a17a6c0c3dc5b94305164096_101605_3d4b5db43492ea1e3e3151c7a34470a1.png&#34;
               width=&#34;432&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Filtering criteria to select GitHub repos.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;ol&gt;
&lt;li&gt;Original (not forked) project with Python as the primary language.&lt;/li&gt;
&lt;li&gt;Contains at least one data science keywords like machine-learning, deep neural network in the description of the project. The whole list of keywords are as follows:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;machine learn&amp;quot;, &amp;quot;machine-learn&amp;quot;, &amp;quot;data sci&amp;quot;, &amp;quot;data-sci&amp;quot;, &amp;quot;big data&amp;quot;, &amp;quot;big-data&amp;quot;,
 &amp;quot;large data&amp;quot;, &amp;quot;large-data&amp;quot;, &amp;quot;data analy&amp;quot;, &amp;quot;data-analy&amp;quot;, &amp;quot;deep learn&amp;quot;, &amp;quot;deep-learn&amp;quot;,
 &amp;quot;data model&amp;quot;, &amp;quot;data-model&amp;quot;, &amp;quot;artificial intel&amp;quot;, &amp;quot;artificial-intel&amp;quot;, &amp;quot;mining&amp;quot;,
  &amp;quot;topic modelling&amp;quot;, &amp;quot;topic-modelling&amp;quot;, &amp;quot;natural language pro&amp;quot;, &amp;quot;natural-language-pro&amp;quot;,
  &amp;quot;nlp&amp;quot;, &amp;quot;data frame&amp;quot;, &amp;quot;data proces&amp;quot;, &amp;quot; ml &amp;quot;, &amp;quot;tensorflow&amp;quot;, &amp;quot;tensor flow&amp;quot;, &amp;quot;tensor-flow&amp;quot;,
  &amp;quot;theano&amp;quot;, &amp;quot;caffe&amp;quot;, &amp;quot;keras&amp;quot;, &amp;quot;scikit-learn&amp;quot;, &amp;quot;kaggle&amp;quot;, &amp;quot;spark&amp;quot;, &amp;quot;hadoop&amp;quot;, &amp;quot;mapreduce&amp;quot;,
  &amp;quot;hdfs&amp;quot;, &amp;quot;neural net&amp;quot;, &amp;quot;neural-net&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Contains at least one usage of data science library like Pytorch, Caffe, Keras, Tensorflow, etc. A full list of used 33 Python data science libraries are listed below:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;theano&amp;quot;, &amp;quot;pytroch&amp;quot;, &amp;quot;caffe&amp;quot;, &amp;quot;keras&amp;quot;, &amp;quot;tensorflow&amp;quot;, &amp;quot;sklearn&amp;quot;, &amp;quot;numpy&amp;quot;, &amp;quot;scipy&amp;quot;, &amp;quot;pandas&amp;quot;, &amp;quot;statsmodels&amp;quot;,
&amp;quot;matplotlib&amp;quot;, &amp;quot;seaborn&amp;quot;, &amp;quot;plotly&amp;quot;, &amp;quot;bokeh&amp;quot;, &amp;quot;pydot&amp;quot;, &amp;quot;xgboost&amp;quot;, &amp;quot;catboost&amp;quot;, &amp;quot;lightgbm&amp;quot;, &amp;quot;eli5&amp;quot;,
&amp;quot;elephas&amp;quot;, &amp;quot;spark&amp;quot;, &amp;quot;nltk&amp;quot;, &amp;quot;cntk&amp;quot;, &amp;quot;scrapy&amp;quot;, &amp;quot;gensim&amp;quot;, &amp;quot;pybrain&amp;quot;, &amp;quot;lightning&amp;quot;, &amp;quot;spacy&amp;quot;, &amp;quot;pylearn2&amp;quot;,
&amp;quot;nupic&amp;quot;, &amp;quot;pattern&amp;quot;, &amp;quot;imblearn&amp;quot;, &amp;quot;pyenv&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Each repository contains at least 80 star.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The dataset contains projects owned by both organizations and individual users. Some of the top rated projects are &lt;em&gt;Tensorflow Models, Keras, Scikit-learn, Pandas, Spacy, Spotify Luigi, NVIDIA FastPhotoStyle, Theano,&lt;/em&gt; etc.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    A full list of all the 1,558 Github projects are available &lt;a href=&#34;https://github.com/boalang/MSR19-DataShowcase/blob/master/info.txt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;350 projects in the dataset are maintained by different organizations (Google, Microsoft, NVIDIA etc.). &lt;a href=&#34;https://github.com/boalang/MSR19-DataShowcase/blob/master/List-of-Organization.txt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The list of organizations is here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The rest 1,208 projects are maintained by individual users. &lt;a href=&#34;https://github.com/boalang/MSR19-DataShowcase/blob/master/List-of-Individual-Users.txt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The list of users is here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;availability&#34;&gt;Availability&lt;/h2&gt;
&lt;p&gt;The dataset is available in Boa infrastructure. Go to the &lt;a href=&#34;http://boa.cs.iastate.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boa web interface&lt;/a&gt; and login. If you do not have an account, you can request a user.&lt;/p&gt;
&lt;p&gt;Then click on the &lt;a href=&#34;http://boa.cs.iastate.edu/boa/index.php?q=boa/run&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;Run Examples&lt;/code&gt;&lt;/a&gt; menu and select &lt;code&gt;2020 August/Python-DS&lt;/code&gt; in the input dataset dropdown option. Finally, you can paste the Boa code and mine desired information.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    The code, dataset details, and sample Boa queries are &lt;a href=&#34;https://github.com/boalang/MSR19-DataShowcase&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shared in this repository&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;p&gt;To use the dataset go to &lt;a href=&#34;http://boa.cs.iastate.edu&#34;&gt;Boa website&lt;/a&gt; and
follow the steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;From the left menu, select User Login to login as a registered user. If you
are not registered, request for a user.&lt;/li&gt;
&lt;li&gt;Write a query under the Boa Source Code. If researchers are not familiar
with the language, the example Boa programs can be utilized by clicking the
Select Examples. Some good examples for this dataset can be also found from the
&lt;a href=&#34;https://github.com/boalang/MSR19-DataShowcase/tree/master/Boa_Queries&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Select &lt;code&gt;2020 August/Python-DS&lt;/code&gt; dataset in the drop-down list under Input
Dataset and run the query.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The job will be submitted to Hadoop cluster and is executed parallely on the
dataset. When the job status is finished, the output text file will be available
for downloading. The job is saved for future reference. One can share the job
with others and one can reproduce the result.&lt;/p&gt;
&lt;p&gt;To learn about Boa language and queries, navigate through the Boa website,
especially &lt;a href=&#34;http://boa.cs.iastate.edu/docs/index.php&#34;&gt;Programming Guide Section&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Update: We have improved the framework to parse and mine Jupyter Notebooks along with the Python files.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Quantifying Uncertainty of DNN Hyperparameter Optimization using a First Order Type</title>
      <link>https://sumonbis.github.io/academic-project/lambda-calculus/</link>
      <pubDate>Wed, 11 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/academic-project/lambda-calculus/</guid>
      <description>&lt;p&gt;Hyperparameter optimization is a difficult problem in developing deep learning applications. Recently, random search based strategies have been proven efficient for optimizing hyperparameters. However, programmers can not overtly represent uncertainty of the chosen hyperparameter values and accuracy of the model while performing a random search. In this project, we utilize a first order type &lt;code&gt;Uncertain&amp;lt;T&amp;gt;&lt;/code&gt; to approximate the distributions of the hyperparameters so that programmers can pick values with certain confidence. This type helps us to represent uncertainty of random hyperparameters and allows us to easily propagate this uncertainty through computations, perform statistical tests on distributions without resorting to complicated statistical concepts, and determine uncertain hyperparameter value in required significance level.&lt;/p&gt;
&lt;p&gt;To the best of our knowledge, there has not been any attempt to introduce the probabilistic programming concept in DNN hyperparameter optimization. The contributions of this project are as follows. First, we have implemented the first order type &lt;code&gt;Uncertain&amp;lt;T&amp;gt;&lt;/code&gt; to hold the distribution of loss values over the randomly chosen hyperparameters. The main goal is to help programmers overtly represent uncertainty in chosen hyperparameters and make conditional statements using that. By using this type, we define algebra over random variables so that the uncertainty of the hyperparameters can propagate through the calculations and provide convergence speed and increase in accuracy. Second, our method performs significantly better than the random search method which is used by most of the DNN libraries. Our result shows that while 62% of the random search trials fall below the accuracy threshold, only 23% time our method fall below the threshold.&lt;/p&gt;
&lt;p&gt;In this project, our goal is to aid the deep learning programmers to quantify uncertainty in the model hyperparameters and make an informed decision while initializing hyperparameters. A problem with the random search is that it doesn’t take uncertainty of random hyperparameters into account while picking a best value which may adversely impact models when trained on the different distribution of the input domain. To that end, we have leveraged a first order type Uncertain&lt;T&gt; to represent the uncertainty in the random hyperparameters and choose best value by performing statistical tests on the distribution. The main contributions of the project are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;We have introduced the probabilistic programming concept in DNN hyperparameter optimization.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We have utilized a first order type &lt;a href=&#34;https://doi.org/10.1145/2541940.2541958&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;Uncertain&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/a&gt; to approximate the distributions over the possible hyperparameter values.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Describe the algebra to perform computations over the uncertain hyperparameters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provide syntax to ask boolean question on the uncertain data type to control false
positive and false negative.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Improve the performance of random search for hyperparameter optimization.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Boa Meets Python: A Boa Dataset of Data Science Software in Python Language</title>
      <link>https://sumonbis.github.io/publication/msr19/</link>
      <pubDate>Sun, 26 May 2019 21:25:33 -0500</pubDate>
      <guid>https://sumonbis.github.io/publication/msr19/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Large-Scale Mining of Data-Science Software from GitHub</title>
      <link>https://sumonbis.github.io/project/mining-ml/</link>
      <pubDate>Tue, 16 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/project/mining-ml/</guid>
      <description>&lt;p&gt;The usage of data science (DS) techniques have increased immensely in the recent past. With the development of artificial intelligence (AI) and machine learning (ML) algorithms and availability of huge amount of data, there has been a rapid increase of using DS components in software. To enable further software engineering (SE) research of DS software in the wild, we built an infrastructure to mine and analyze DS software in large-scale. Our goal is to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn from the past and guide future development of DS software&lt;/li&gt;
&lt;li&gt;Improve software design and reuse&lt;/li&gt;
&lt;li&gt;Manage DS software better&lt;/li&gt;
&lt;li&gt;Automatic bug detection and repair&lt;/li&gt;
&lt;li&gt;Many more &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We think that software engineering research for DS systems is necessary. To that end, we leveraged the huge amount of code and metadata available in open-source GitHub repositories. We proposed a methodology of filtering high-quality DS repositories and then mined code of each revision and project metadata e.g., number of developers, commit logs, creation-date, etc.&lt;/p&gt;
&lt;!-- 













&lt;figure  id=&#34;figure-mined-data-from-github-ds-repositories&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;dataset.png&#34; alt=&#34;Mined data from GitHub DS repositories&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Mined data from GitHub DS repositories
    &lt;/figcaption&gt;&lt;/figure&gt; --&gt;
&lt;p&gt;We used the existing &lt;a href=&#34;http://boa.cs.iastate.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boa&lt;/a&gt; framework to mine from GitHub. Python has been used as a de-facto of ML based software for a long time now. However, Boa could not parse and mine Python code. Therefore, I built the infrastructure to parse the Python code, transform that into AST, and store in Boa&amp;rsquo;s Protobuf format for storing. I used &lt;a href=&#34;https://www.antlr.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ANTLR&lt;/a&gt; grammar for Python 2 and Python 3 for parsing the source. Finally, the dataset is stored in the Hadoop cluster for further analysis. You can learn about how to use the Boa for mining new datasets from &lt;a href=&#34;https://sumonbis.github.io/post/boa-tutorial/&#34;&gt;this tutorial&lt;/a&gt;. The first version of the dataset contained about 5 million Python files (including all revisions) from top rated 1558 DS repositories.&lt;/p&gt;














&lt;figure  id=&#34;figure-the-first-version-of-the-mined-dataset&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The first version of the mined dataset&#34; srcset=&#34;
               /project/mining-ml/data_hu10570a67aa14b827085367c50d3935f2_71196_72ff55399386f4a597998fe540e0d2a9.png 400w,
               /project/mining-ml/data_hu10570a67aa14b827085367c50d3935f2_71196_6001210ad4ebce11c638de78026a1845.png 760w,
               /project/mining-ml/data_hu10570a67aa14b827085367c50d3935f2_71196_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/project/mining-ml/data_hu10570a67aa14b827085367c50d3935f2_71196_72ff55399386f4a597998fe540e0d2a9.png&#34;
               width=&#34;288&#34;
               height=&#34;344&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The first version of the mined dataset
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The rich amount of data facilitates analyzing several research questions:&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Data science development practices:&lt;/mark&gt; Due to the recent ‘boom’ in machine learning software development, a lot of questions regearing DS best practices can be asked:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What are the most used data preprocessing library?&lt;/li&gt;
&lt;li&gt;Why data dimension bugs are more frequent in deep neural networks?&lt;/li&gt;
&lt;li&gt;What data visualization APIs are most used in last two years?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;mark&gt;Software change:&lt;/mark&gt; Since the dataset contains all the snapshots of Python files, how developers change source can be studied.&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;Program analysis:&lt;/mark&gt; The differences between traditional and DS software development can be studied. For example, do the code complexity or API usages follow similar pattern in DS software?&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;SE practices:&lt;/mark&gt; Several SE research can be conducted on the DS projects in the area of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DS bug detection and repair&lt;/li&gt;
&lt;li&gt;Code comprehension&lt;/li&gt;
&lt;li&gt;Testing DS modules&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;mark&gt;Project metadata:&lt;/mark&gt; The project metadata can be leveraged to answer questions such as, what are the coding style differences between projects developed by individuals and organizations, etc.&lt;/p&gt;
&lt;p&gt;Further details about the dataset can be found here. The domain-specific Boa language helps to perform program analysis query the metadata easily and quickly. For example, the following code snippet quickly count the number of bug-fixing revisions for all the projects in the dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Counting the number of bug-fixing revisons for all projects
p: Project = input;
counts: output sum[string] of int;

visit(p, visitor {
	before n: Project -&amp;gt; ;
	before node: CodeRepository -&amp;gt; ;
	before node: Revision -&amp;gt;
		if (isfixingrevision(node.log))
			counts[p.name] &amp;lt;&amp;lt; 1;
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want to go deeper and mine the all the method calls in the bug-fixing revisions, we can use the following Boa program.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;a href=&#34;http://boa.cs.iastate.edu/boa/?q=boa/job/public/88574&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Example Boa program&lt;/a&gt; for mining method calls in the bug fixing commits.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Note that the output contains 74.11 million results. We use commit logs to identify whether it&amp;rsquo;s a bug-fixing revision or not and then visit the expression to get the method calls. Boa uses automatic parallelization to run the query in Haddop cluster. Users do not need to worry about the storage and low-level details.&lt;/p&gt;
&lt;p&gt;As of this writing, no open source dataset for studying Data Science software is available. We created a infrastructure and dataset to mins DS projects from GitHub that are using Python.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Semantics of Compound Comparison Statement and Generator Function in Lambda Calculus</title>
      <link>https://sumonbis.github.io/academic-project/dnn-uncertainty/</link>
      <pubDate>Fri, 22 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/academic-project/dnn-uncertainty/</guid>
      <description>&lt;p&gt;Python is a widely used programming language in education, science and industry. Many distinctive language constructs made Python easy-to-learn and expressive. However, semantics of some advanced features can create corners for the language such as weak scope resolution. The language weaknesses lead to different behaviors from IDEs and create confusions among developers. In this project, our goal is to study two unique language constructs of Python: compound comparison statement and generator. Compound comparison statement is similar to regular if statement but it contains chaining comparison as conditions. Chaining comparison (e.g., x &amp;lt; 10 &amp;lt; x*10 &amp;lt; 100) is a syntactic sugar which combines two comparison operations into one. Generator is another powerful control-flow construct with one or more yield statements which is used for creating user-defined iterators. In this study, we have extended Lambda cal- culus to implement a core language that includes the above two Python language features. We have also described the details of the syntax, operational semantics and type system of the features. In our implementation, we have used Coq proof assistant so that we can further check the correctness of the construct properties.&lt;/p&gt;
&lt;p&gt;For details refer to the project &lt;a href=&#34;https://github.com/sumonbis/NewSemantics/blob/master/report.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;report&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Boa for Big-Code Mining and Large-Scale Static Analysis</title>
      <link>https://sumonbis.github.io/post/boa-tutorial/</link>
      <pubDate>Fri, 13 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/post/boa-tutorial/</guid>
      <description>&lt;p&gt;This tutorial will describe how to create your own Boa dataset for specific projects in Github and run Boa queries on that dataset locally. We will use command line and Eclipse IDE for that purpose.&lt;/p&gt;
&lt;h2 id=&#34;prerequisite&#34;&gt;Prerequisite&lt;/h2&gt;
&lt;p&gt;You need to have following already installed in your system:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;JDK&lt;/li&gt;
&lt;li&gt;Apache Ant&lt;/li&gt;
&lt;li&gt;Git&lt;/li&gt;
&lt;li&gt;Eclipse IDE&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;development-setup-steps&#34;&gt;Development Setup Steps&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Clone the Boa project using the command line: &lt;code&gt;git clone https://github.com/boalang/compiler.git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Go to the cloned repository: &lt;code&gt;cd compiler&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Clean the project: &lt;code&gt;ant clean&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create a directory for libraries: &lt;code&gt;mkdir -p build/classes&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Compile the project: &lt;code&gt;ant compile&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create a class folder: &lt;code&gt;mkdir compile&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;In Eclipse go to: File &amp;gt; Open Projects from File System &amp;gt; Import Source – Directory &amp;gt; Browse the cloned repository (compiler) &amp;gt; Hit Finish&lt;/li&gt;
&lt;/ol&gt;














&lt;figure  id=&#34;figure-import-compiler-project-in-eclipse&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Import compiler project in Eclipse&#34; srcset=&#34;
               /post/boa-tutorial/import_hu18ade01ed00817a4bb1fa04cbf19de50_100306_8b8e2b26342e735bcc4f07d6f49124d5.png 400w,
               /post/boa-tutorial/import_hu18ade01ed00817a4bb1fa04cbf19de50_100306_36ddbde6110206d333a9f39ed372eb8e.png 760w,
               /post/boa-tutorial/import_hu18ade01ed00817a4bb1fa04cbf19de50_100306_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/post/boa-tutorial/import_hu18ade01ed00817a4bb1fa04cbf19de50_100306_8b8e2b26342e735bcc4f07d6f49124d5.png&#34;
               width=&#34;760&#34;
               height=&#34;529&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Import compiler project in Eclipse
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;!-- &lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;img/import.png&#34; title=&#34;Import compiler project in Eclipse&#34;&gt;
&lt;/p&gt; --&gt;
&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;Right click on the project compiler &amp;gt; Build Path &amp;gt; Configure Build Path&lt;/li&gt;
&lt;li&gt;In Source tab, click Add Folder to add the required source folders and remove unnecessary folder(s). After adding all the folders, the window should look like the following:&lt;/li&gt;
&lt;/ol&gt;














&lt;figure  id=&#34;figure-after-configuring-eclipse&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;After configuring Eclipse&#34; srcset=&#34;
               /post/boa-tutorial/after_config_huc30bb9b6770fa560d57158247648e19e_254071_0f267088009d39f4d6144eed5cf2e29e.png 400w,
               /post/boa-tutorial/after_config_huc30bb9b6770fa560d57158247648e19e_254071_8beb014657c4a32573ee7449d578aefd.png 760w,
               /post/boa-tutorial/after_config_huc30bb9b6770fa560d57158247648e19e_254071_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/post/boa-tutorial/after_config_huc30bb9b6770fa560d57158247648e19e_254071_0f267088009d39f4d6144eed5cf2e29e.png&#34;
               width=&#34;760&#34;
               height=&#34;438&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      After configuring Eclipse
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;!-- &lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;img/after_config.png&#34; title=&#34;After configuring Eclipse&#34;&gt;
&lt;/p&gt; --&gt;
&lt;ol start=&#34;10&#34;&gt;
&lt;li&gt;Select Libraries tab in the same window and click on Add Class Folder and add the compiler/compile folder that has been created in step 6.&lt;/li&gt;
&lt;li&gt;Click Add JARs… in the same Libraries tab &amp;gt; select lib &amp;gt; select all the files inside lib, including files under datagen and evaluator folder &amp;gt; hit Apply and Close.&lt;/li&gt;
&lt;li&gt;From the compiler project in Eclipse, right click on build.xml &amp;gt; Run as &amp;gt; 1 Ant build. This should build the project successfully. The development setup is completed. Now, we will move on to data generation steps.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;boa-data-generation-steps&#34;&gt;Boa Data Generation Steps&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Go to github.com and search the project for which you want to create the dataset. For example, if you want to create dataset for Apache Mavan project, go to &lt;a href=&#34;https://github.com/apache/maven&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/apache/maven&lt;/a&gt; .&lt;/li&gt;
&lt;li&gt;Invoke a GitHub http-based RESTful API to get the metadata of the project by constructing a URL &lt;a href=&#34;https://api.github.com/repos/repo_full_name&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://api.github.com/repos/repo_full_name&lt;/a&gt;, for example &lt;a href=&#34;https://api.github.com/repos/apache/maven&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://api.github.com/repos/apache/maven&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Copy the whole JSON metadata, create a blank text file, type a pair of brackets ‘[]’, paste the metadata inside the brackets.&lt;/li&gt;
&lt;li&gt;Search for &amp;ldquo;languages_url&amp;rdquo; field in the JSON data, go to the URL, copy everything, create another field in the JSON file (&amp;ldquo;language_list&amp;rdquo;: ), paste copied text and save the file as filename.json (e.g., maven.json). The last few lines of the JSON file should look like:&lt;/li&gt;
&lt;/ol&gt;














&lt;figure  id=&#34;figure-json-file&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;JSON file&#34; srcset=&#34;
               /post/boa-tutorial/json_looklike_hu2ab2130319f6cfb527ad0ef2986ae93a_168440_f49bec1fdd27f7b5c178995e13009bd9.png 400w,
               /post/boa-tutorial/json_looklike_hu2ab2130319f6cfb527ad0ef2986ae93a_168440_20af3c33d6658053b2194ed22c8b5fb8.png 760w,
               /post/boa-tutorial/json_looklike_hu2ab2130319f6cfb527ad0ef2986ae93a_168440_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/post/boa-tutorial/json_looklike_hu2ab2130319f6cfb527ad0ef2986ae93a_168440_f49bec1fdd27f7b5c178995e13009bd9.png&#34;
               width=&#34;600&#34;
               height=&#34;395&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      JSON file
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;!-- &lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;img/json_looklike.png&#34; title=&#34;JSON file&#34;&gt;
&lt;/p&gt; --&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;In this way, for each project, that will be included in the dataset, create a JSON file. So, if you want to create a dataset for 5 projects, you will create 5 separate JSON files. Save all the JSON files in a folder. Alternatively, you can create a single JSON file for all of the projects by separating them by comma in an array ‘[]’. The format looks like [{}, {}]. Each curly brace is for one project.&lt;/li&gt;
&lt;li&gt;In Eclipse, go to the project compiler &amp;gt; src/java &amp;gt; boa.datagen, right click on BoaGenerator.java &amp;gt; Run As &amp;gt; Run Configurations.&lt;/li&gt;
&lt;li&gt;Double click on Java Application to create a new configuration. Browse project and select compiler, Search Main Class and select boa.datagen.BoaGenerator.&lt;/li&gt;
&lt;/ol&gt;














&lt;figure  id=&#34;figure-data-generator-configuration&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Data generator configuration&#34; srcset=&#34;
               /post/boa-tutorial/datagen_hu949fdfc0778c85be9ad23ef2b524f8af_286100_975fb2b00653598ae3d0aaaf66cd565c.png 400w,
               /post/boa-tutorial/datagen_hu949fdfc0778c85be9ad23ef2b524f8af_286100_a821359fabc7942f9039a41f15b7635b.png 760w,
               /post/boa-tutorial/datagen_hu949fdfc0778c85be9ad23ef2b524f8af_286100_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/post/boa-tutorial/datagen_hu949fdfc0778c85be9ad23ef2b524f8af_286100_975fb2b00653598ae3d0aaaf66cd565c.png&#34;
               width=&#34;718&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Data generator configuration
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;Select Arguments tab and add program arguments. The program arguments format should look like:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;  -inputJson 	&amp;lt;directory containing project JSON files&amp;gt;
  -output 	&amp;lt;output directory of the dataset (folder will be automatically created)&amp;gt;
  -inputRepo 	&amp;lt;temporary directory used to clone the projects (this folder will be automatically created)&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The other arguments are optional. For example, to print debug messages in console use -debug.&lt;/p&gt;

 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;figure  id=&#34;figure-data-generator-parameters&#34;&gt;
   &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
     &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Data generator parameters&#34; srcset=&#34;
                /post/boa-tutorial/datagenparam_hu0ab8dbc86191e859d9cf2dbaf834ceed_190851_9bcc7799a517c20c420a928e34056f90.png 400w,
                /post/boa-tutorial/datagenparam_hu0ab8dbc86191e859d9cf2dbaf834ceed_190851_f15015d26c48b89e9bc0530707a356ba.png 760w,
                /post/boa-tutorial/datagenparam_hu0ab8dbc86191e859d9cf2dbaf834ceed_190851_1200x1200_fit_lanczos_3.png 1200w&#34;
                src=&#34;https://sumonbis.github.io/post/boa-tutorial/datagenparam_hu0ab8dbc86191e859d9cf2dbaf834ceed_190851_9bcc7799a517c20c420a928e34056f90.png&#34;
                width=&#34;760&#34;
                height=&#34;650&#34;
                loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
   &lt;/div&gt;&lt;figcaption&gt;
       Data generator parameters
     &lt;/figcaption&gt;&lt;/figure&gt;
&lt;ol start=&#34;9&#34;&gt;
&lt;li&gt;Hit Run.&lt;/li&gt;
&lt;li&gt;This should start cloning the projects form Github and generating dataset. Depending on the number of projects and size of the projects, this will take some time to finish. When the red Terminate option in the console goes off, the data generation process is finished.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;run-boa-query-on-new-dataset&#34;&gt;Run Boa Query on New Dataset&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Create a dataset folder copying three files (projects.seq, ast/data, ast/index) from the generated output folder from step 8 of data generation process.&lt;/li&gt;
&lt;li&gt;In Eclipse, go to the project compiler &amp;gt; src/java &amp;gt; boa.evaluator, right click on BoaEvaluator.java &amp;gt; Run As &amp;gt; Run Configurations…&lt;/li&gt;
&lt;li&gt;Create a new configuration by clicking the New Configuration in the upper left corner of the window.&lt;/li&gt;
&lt;li&gt;Give a Name to the configuration, Browse project and select compiler, Search Main Class and select boa.evaluator.BoaEvaluator.&lt;/li&gt;
&lt;li&gt;Select Arguments tab and add program arguments. The program arguments format should look like:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;-input &amp;lt;file path to the boa source code file&amp;gt;
-data &amp;lt;dataset directory containing three files(projects.seq, data, index)&amp;gt;
-output &amp;lt;output directory&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;figure  id=&#34;figure-data-evaluator-parameters&#34;&gt;
   &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
     &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Data evaluator parameters&#34; srcset=&#34;
                /post/boa-tutorial/evalparam_hue8c49e7c5cd92ffdb7d8309808005f8d_259498_bd1849e8859a8e0de8f2553ea8b570e1.png 400w,
                /post/boa-tutorial/evalparam_hue8c49e7c5cd92ffdb7d8309808005f8d_259498_38c40ac3b74c73d956959061788cdf42.png 760w,
                /post/boa-tutorial/evalparam_hue8c49e7c5cd92ffdb7d8309808005f8d_259498_1200x1200_fit_lanczos_3.png 1200w&#34;
                src=&#34;https://sumonbis.github.io/post/boa-tutorial/evalparam_hue8c49e7c5cd92ffdb7d8309808005f8d_259498_bd1849e8859a8e0de8f2553ea8b570e1.png&#34;
                width=&#34;760&#34;
                height=&#34;467&#34;
                loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
   &lt;/div&gt;&lt;figcaption&gt;
       Data evaluator parameters
     &lt;/figcaption&gt;&lt;/figure&gt;
 &lt;!-- &lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;img/evalparam.png&#34; title=&#34;Data evaluator parameters&#34;&gt;
&lt;/p&gt; --&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;The output of the query will be printed in the console.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The screencast to go over the above steps and setup Boa development environment is shown in the following video.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/vuHUx-NYrOo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Computational Thinking of K-12 Outreach Program</title>
      <link>https://sumonbis.github.io/post/k-12-coding-competition-judge/</link>
      <pubDate>Sat, 14 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/post/k-12-coding-competition-judge/</guid>
      <description>&lt;p&gt;As part of the K-12 Outreach program, ISU computer science organized Computational Thinking Event this year. I got the opportunity to be a judge of the competition and see the outstanding computational projects developed by bright youngsters!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Science fair-like format where participants come prepared
to present their projects to a panel of experts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Computational thinking is critical skill that is needed for the success in the 21st century. The exposure to problem solving skills at an early stage helps the students greatly in their future endeavor. Since Fall 2010, the ISU Computer Science Department has been making efforts in Iowa K-12 schools with a series of events and workshops designed to help K-12 educators, parents, and students learn about computational thinking.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Trophies, Prizes, Awards of Excellence, and Certificates of
Participation. Prizes included laptops, tablets and Raspberry PIs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This particular event was last Saturday at Iowa State campus. The computer faculties who were involved in the outreach program are: Wallapak Tavanapong, Soma Chaudhuri, Yan-Bin Jia, Lu Ruan, Simanta Mitra, and Oliver Eulenstein.&lt;/p&gt;
&lt;p&gt;I joined the panel of judges along with other graduate students. I was really amazed to see how interesting and skillful projects were developed by the young students. The students competed in the following groups:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;K 3&lt;/li&gt;
&lt;li&gt;K 4-6&lt;/li&gt;
&lt;li&gt;K 7-9&lt;/li&gt;
&lt;li&gt;K 10-12&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Contestants could work in small groups of two or by themselves. The judgements were mainly based on the difficulty of the problem, cleverness of the solution, and ability of the students to explain.&lt;/p&gt;
&lt;p&gt;Most of the younger students were using the &lt;a href=&#34;https://scratch.mit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scratch platform developed in MIT&lt;/a&gt;, to learn and build interesting games. Scratch is designed and maintained by the Lifelong Kindergarten group at the MIT Media Lab. Here is an example project &lt;a href=&#34;https://scratch.mit.edu/projects/211107373/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Speech Helper 2.0&lt;/a&gt; built by Carson and McClain Crigger, both sixth graders at South Middle School in Waukee, IA. The program is designed to help their younger brother practice letter sounds.&lt;/p&gt;
&lt;iframe src=&#34;https://scratch.mit.edu/projects/211107373/embed&#34; allowtransparency=&#34;true&#34; width=&#34;485&#34; height=&#34;402&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;blockquote&gt;
&lt;p&gt;Scratch is a programming language and an online community where children can program and share interactive media such as stories, games, and animation with people from all over the world.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It was fascinating to see the coding skill of the kids and how they are engaged in the activity. There were three projects in the K 10-12 group which were solving complex problem with exciting ideas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Project &lt;strong&gt;Mardiop&lt;/strong&gt; by Adam&lt;/li&gt;
&lt;li&gt;Project &lt;strong&gt;Electric Field Simulation&lt;/strong&gt; by Brandt&lt;/li&gt;
&lt;li&gt;Project &lt;strong&gt;Fast Intersection&lt;/strong&gt; by Prithvi&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Those projects were rich in formulating the problem, organizing and analyzing data, and testing and managing their code. It was a great day to talk to them and share ideas.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Naive Bayes Classifier for Text Documents</title>
      <link>https://sumonbis.github.io/academic-project/naive-bayes/</link>
      <pubDate>Sun, 05 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/academic-project/naive-bayes/</guid>
      <description>&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;I have implemented Java program that takes the six files as input, builds a Naive Bayes classifier and outputs relevant statistics. I have built the Naive Bayes classifier from the training data (train label.csv, train data.csv), then evaluated its performance on the testing data (test label.csv, test data.csv).&lt;/p&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;The 20 Newsgroups dataset is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. It was originally collected by Ken Lang, probably for his Newsweeder: Learning to filter netnews[1] paper.The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering. The data is organized into 20 different newsgroups, each corresponding to a different topic.&lt;/p&gt;
&lt;p&gt;The original data set is available at &lt;a href=&#34;http://qwone.com/~jason/20Newsgroups/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://qwone.com/~jason/20Newsgroups/&lt;/a&gt;. It includes 18824 documents which have been divided to two subsets: training (11269 documents) and testing (7505 documents). The vocabulary.txt contains all distinct words and other tokens in the 18824 documents. train data.csv and test data.csv are formatted &amp;ldquo;docIdx, wordIdx, count&amp;rdquo;, where docIdx is the document id, wordIdx represents the word id (in correspondence to vocabulary.txt) and count is the frequency of the word in the document. train label.csv and test label.csv are simply a list of label id’s indicating which newsgroup each document belongs to. The map.csv maps from label id’s to label names.&lt;/p&gt;
&lt;p&gt;Instructions to run the program:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Copy all *.java files to one directory.&lt;/li&gt;
&lt;li&gt;Place the data files in the same directory&lt;/li&gt;
&lt;li&gt;Use command line to run.
i. cd to the directory.
ii. Compile : $ javac *.java
iii. Run: $ java NaiveBayes vocabulary.txt map.csv train_label.csv train_data.csv test_label.csv test_data.csv&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The detail results and output can be found here: &lt;a href=&#34;https://github.com/sumonbis/NaiveBayesClassifier/blob/master/Result.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/sumonbis/NaiveBayesClassifier/blob/master/Result.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Ken Lang, Newsweeder: Learning to filter netnews, Proceedings of the Twelfth International Conference on Machine Learning, 331-339 (1995).&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Near Duplicate Detection Using Simhash</title>
      <link>https://sumonbis.github.io/academic-project/simhash/</link>
      <pubDate>Tue, 10 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/academic-project/simhash/</guid>
      <description>&lt;p&gt;Near duplicate detection in a large collection of files is a well-studied problem in data science. Many Locality Sensitive Hashing (LSH) algorithms have been recently developed to solve this problem. Among them simhash is a very efficient LSH algorithm that uses probabilistic method to generate similar fingerprints for similar objects. In this project, we have implemented simhash algorithm to evaluate approximate cosine similarity between two documents from a large collection of files. We have preprocessed the documents, created word vectors with weight and then implemented simhash algorithm to generate 64-bit fingerprint of each document. Then we have implemented block permuted hamming search in our fingerprint space. Block permuted Hamming search helps us to reduce the time to find similar pairs significantly. However, we have to consider a few false negatives in this result. By designing the block permutation in a better way, we can reduce the false negative rate.&lt;/p&gt;
&lt;p&gt;Detail project report: &lt;a href=&#34;https://github.com/sumonbis/NearDuplicateDetection/blob/master/Report.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/sumonbis/NearDuplicateDetection/blob/master/Report.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;problem-definition&#34;&gt;Problem Definition&lt;/h2&gt;
&lt;p&gt;Suppose, we have millions of documents and given a new document we have to find all the near duplicates (e.g., 95% or more similar) from the collection in a reasonable amount of time. We can divide the problem into two parts: how to measure similarity between two documents and how to find the similar documents form a large collection efficiently? Therefore, our goal is to solve the following problems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Given two documents D_a and D_b, what is the similarity measure between them?&lt;/li&gt;
&lt;li&gt;Given a document D_a, find all the documents that are similar to D_a.&lt;/li&gt;
&lt;li&gt;Identify all the pairs in the collection that are near duplicate of each other.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are a few challenges related to the above problems. First, our algorithm should be designed for millions of documents. Second, the files should be compressed enough to fit in memory. Finally, the algorithm must be efficient to find near duplicate in small amount of time.&lt;/p&gt;
&lt;h2 id=&#34;simhash&#34;&gt;Simhash&lt;/h2&gt;
&lt;p&gt;Charikar’s simhash [1] is a dimensionality reduction technique which maps high dimensional documents to very small sized fingerprints. We can compute the Hamming distance of two finger- prints to measure the cosine similarity.&lt;/p&gt;
&lt;h2 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h2&gt;
&lt;p&gt;The basic sketch of using simhash algorithm to measure similarity is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Step 1: Convert the document into set of features associated with weights.&lt;/li&gt;
&lt;li&gt;Step 2: Create f-bit fingerprint for each document.&lt;/li&gt;
&lt;li&gt;Step 3: Calculate Hamming distance between two fingerprints to measure similarity between corresponding documents.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;First, we have to choose features for each document. Feature selection also depends on the application. Word is a very obvious choice as features. We can also choose shingle as feature. A k-shingle is every k-length adjacent set of characters. Consider the sentence - “The earth is moving.” The set of k-shingles for k = 5: {The_e, he_ea, e_ear, _eart, . . .}. If the application requires such similarity measure that demands the order of appearance then shingle can be a good choice as feature. In this project, we used word as our feature. There are some preprocessing before converting them as set of features. We converted the whole document to lowercase as case sensitivity does not contribute to similarity score. Then we have removed the stop words (e.g., a, an, the etc.) and punctuation symbols which are common in every document. Next, the weight of each word is calculated. There are several ways to calculate the weight of each feature. The feature with more weight will contribute more to the similarity score. A very intuitive weight measure is the frequency of each term. The term which appears more in a document carries more weight. We can also use TF-IDF (Term Frequency-Inverse Document Frequency) as weight.
After the preprocessing is done, we create f-bit binary fingerprint of each document using simhash algorithm. The value of f is 32 or 64 in practice. First, each feature is converted to a f-bit binary hash value using a uniformly distributed hash function (e.g., MD5, FNV, Murmur). Then we define a vector of length f, initially with all zero values. Now, we iterate through each bit position (1 to f). If the bit position is 1 then we add the weight and if the bit position is 0 then we subtract the weight. After all the iterations, we get a vector of real values of length f. Finally, if the ith value is negative we convert it to 0, otherwise we convert it to 1. Thus, we get f-bit fingerprint of a document.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Moses S Charikar. Similarity estimation techniques from rounding algorithms. Proceed- ings of the thiry-fourth annual ACM symposium on Theory of computing-ACM, pages 380–388, 2002.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Public Key Encryption (PEKS) with Bloom Filter</title>
      <link>https://sumonbis.github.io/academic-project/encryption-with-bloom-filter/</link>
      <pubDate>Mon, 02 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/academic-project/encryption-with-bloom-filter/</guid>
      <description>&lt;p&gt;Public Key Encryption with Keyword Search (PEKS) is one of the most used method to search keywords over
encrypted data.&lt;/p&gt;
&lt;p&gt;Suppose, Bob is sending email with specific keywords to Alice. Encrypted emails are stored
in the server. Alice wants to search emails with keywords from email server but does not want to allow the
server decrypt any email. The paper on PEKS [http://crypto.stanford.edu/~dabo/papers/encsearch.pdf]
described two algorithms to achieve that goal.&lt;/p&gt;
&lt;p&gt;The first algorithm takes less time and space compared to the second. However, the first one can not guarantee semantic security. The second one is semantically secure. But dictionary attack can help attackers to guess keywords and pose serious damage. I have resolved that issue using a Bloom Filter. The false positives of a bloom filter does not allow to make it susceptible to dictionary attack.&lt;/p&gt;
&lt;p&gt;In this project, I have implemented the second algorithm of PEKS that originates form trapdoor permutations.
Then I have implemented Bloom Filter that is used to search keywords over the hashmap.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Path ORAM (Oblivious Random Access Memory)</title>
      <link>https://sumonbis.github.io/academic-project/path-oram/</link>
      <pubDate>Fri, 13 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/academic-project/path-oram/</guid>
      <description>&lt;p&gt;Path ORAM is a simple oblivious RAM algorithm. While using cloud platform or any other insecure memory, attack can be made using the access pattern. Oblivious RAM is the way to hide the memory access pattern with some extra bandwidth and memory overhead.&lt;/p&gt;
&lt;h2 id=&#34;path-oram-implementation&#34;&gt;Path Oram Implementation&lt;/h2&gt;
&lt;p&gt;Path ORAM uses a binary tree to store all memory blocks. Each node of the tree is
a bucket which can contain a fixed number of block. First, we define all necessary data
structure. The depth of the tree is &lt;code&gt;ceiling(log N)&lt;/code&gt;. The empty blocks are filled with dummy
data. Each leaf node is a distinct branch and each block is mapped to a random branch.
For each operation, we perform read and write through the branch. Since, the blocks
are positioned to different branches, repeated operations do not disclose any information. A
local memory is used to read and re-write the data. Path ORAM uses limited amount of
memory and bandwidth with respect to other oblivious algorithms. This is an implementation described in the following paper: &lt;a href=&#34;https://people.csail.mit.edu/devadas/pubs/PathORam.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://people.csail.mit.edu/devadas/pubs/PathORam.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;security&#34;&gt;Security&lt;/h2&gt;
&lt;p&gt;Path ORAM changes the location of block repeatedly and accesses the whole branch for
a single block. Therefore, the pattern of access is always random. However, the security is
dependent mostly on the random branch selection of the blocks. We used python package
numpy to obtain uniformly distributed random integer.&lt;/p&gt;
&lt;h2 id=&#34;performance&#34;&gt;Performance&lt;/h2&gt;
&lt;p&gt;For each access, we go through the whole path twice, once for reading and again for writing.
So, we need to access twice the depth of the tree. Since, depth is &lt;code&gt;ceiling(log N)&lt;/code&gt;, the performance
also sticks to that.&lt;/p&gt;
&lt;h2 id=&#34;unit-test&#34;&gt;Unit Test&lt;/h2&gt;
&lt;p&gt;The correctness holds because the path we access includes the intended block. And after
each access the block is remapped to another branch so that access to same block does not
repeat the same set of blocks.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Performance Benchmarking for Link Prediction Algorithms in Social Networks</title>
      <link>https://sumonbis.github.io/academic-project/social-network/</link>
      <pubDate>Mon, 15 Aug 2016 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/academic-project/social-network/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;From a given snapshot of a social network database, we can predict whether a person can be potentially connected to another person, by analyzing existing links. We take two datasets (Facebook dataset from &lt;a href=&#34;https://snap.stanford.edu/data/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stanford Large Network Dataset Collection&lt;/a&gt; and bibliography dataset from &lt;a href=&#34;https://dblp.uni-trier.de/xml/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DBLP&lt;/a&gt;) and import that into MySQL, and Neo4J (Graph based DB) to evaluate the metrics for different network topology.&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;For the past one decade, social network has gained a lot of popularity and more users are
making their online presence to connect. Hence, it brings up new challenges for
analyzing data generated from these users. One such analysis is the social connection between
two users. A lot of work has been done in the past with regard to link analysis. From a given
snapshot of a social network database, we can predict for a given person (or the entire network),
the people who she can be potentially connected to, by analyzing her existing links. Although
there are is a lot of effort put into developing new prediction techniques, there is no solid
function for analyzing which database is suited for a particular link analysis method. Link
prediction can be done either for the entire network, or for a small subset of the network graph
centered on a particular user. We consider the latter in this project.&lt;/p&gt;
&lt;p&gt;We take open datasets and import it into MySQL (for relational), and Neo4J (for Graph
based) and evaluate several link metrics. Experimentally, we plan on classifying how
performance varies with respect to metrics for different databases. We also plan to analyze on
how link metrics vary according to the network topology/parameters. We try to improve the
performance of the queries implemented in the referenced paper : “Implementing link-prediction
for social networks in a database system” by Sarah Cohen et al. Our experiment is performed on
eight different real social network datasets taken from SNAP and DBLP databases. Finally, the
results verify that the changes brought about in the neo4J schema and query structure improve
the performance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Secure Data Security Infrastructure for Small Organization in Cloud Computing</title>
      <link>https://sumonbis.github.io/publication/iceeict15b/</link>
      <pubDate>Thu, 21 May 2015 21:25:33 -0500</pubDate>
      <guid>https://sumonbis.github.io/publication/iceeict15b/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Applying Ant Colony Optimization in Software testing to Generate Prioritized Optimal Path and Test Data</title>
      <link>https://sumonbis.github.io/publication/iceeict15a/</link>
      <pubDate>Thu, 21 May 2015 21:25:33 -0500</pubDate>
      <guid>https://sumonbis.github.io/publication/iceeict15a/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Cloud Based Healthcare Application Architecture and Electronic Medical Record Mining: An Integrated Approach to Improve Healthcare System</title>
      <link>https://sumonbis.github.io/publication/iccit14/</link>
      <pubDate>Mon, 22 Dec 2014 21:25:33 -0500</pubDate>
      <guid>https://sumonbis.github.io/publication/iccit14/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Doctors Window (A solution for Doctors and Patients)</title>
      <link>https://sumonbis.github.io/academic-project/doctors-window/</link>
      <pubDate>Sun, 20 Apr 2014 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/academic-project/doctors-window/</guid>
      <description>&lt;p&gt;This project contains one desktop app (for doctors) and two client side app (for doctors and patients).&lt;/p&gt;
&lt;h2 id=&#34;desktop-app&#34;&gt;Desktop App&lt;/h2&gt;
&lt;p&gt;A windows application software has been built which will be installed on doctors’ computer. This software is a standalone one and can be maintained by one single doctor.&lt;/p&gt;
&lt;p&gt;The doctor will enter every patient’s information and generate prescription using this software. Then the prescription can be printed and handed over to patient. One copy electronic prescription will be sent patient’s email address as well. The appointment system will be maintained by the system using SMS. Two basic operation are handled by the software:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Electronic prescription module&lt;/li&gt;
&lt;li&gt;Patient scheduling and queue management using SMS&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/sumonbis/DoctorsWindow/master/Desktop%20App/screens/batch2.png&#34; alt=&#34;Desktop app&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;electronic-prescription-module&#34;&gt;Electronic Prescription Module&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Electronic prescription handling&lt;/li&gt;
&lt;li&gt;Medicine suggestion&lt;/li&gt;
&lt;li&gt;Store all prescription&lt;/li&gt;
&lt;li&gt;Print prescription&lt;/li&gt;
&lt;li&gt;Email prescription&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/sumonbis/DoctorsWindow/master/Desktop%20App/screens/batch1.png&#34; alt=&#34;Desktop app&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;patients-scheduling-and-queue-management-using-sms&#34;&gt;Patient’s Scheduling and Queue Management Using SMS&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Make appointment sending SMS&lt;/li&gt;
&lt;li&gt;Reply sending date and time to patients&lt;/li&gt;
&lt;li&gt;The system maintains the patient list and calendar&lt;/li&gt;
&lt;li&gt;Any greeting or emergency message can be sent to the patients&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;used-tools-and-technologies&#34;&gt;Used Tools and Technologies&lt;/h4&gt;
&lt;p&gt;This is a windows based application developed on .Net Platform. The windows form is built on Visual Studio 2012. SQL Server 2008 is used for database support. Here are the technology specifications:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Visual Studio 2012 2. .Net 4.5&lt;/li&gt;
&lt;li&gt;C#&lt;/li&gt;
&lt;li&gt;SQL Server 2008&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;AT Command: AT commands are used to control GSM or GPRS modems. This command can be executed by the modems to perform particular tasks like send- ing SMS to a number, receiving and save message to SIM memory, deleting SMS etc. Actually, it performs operation on the SIM which is mounted on the modem. The command can connect modems to specific ports of the desktop computer and perform tasks. C# language was used to manipulate the AT commands.&lt;/p&gt;
&lt;p&gt;Crystal report SAP: There are different crystal reporting system which can be installed on Visual Studio to provide report querying from database. RDLC, SAP are mostly used. Electronic prescription reporting is provided by SAP Crystal Report 13. The SAP Crystal Report allows one to generate prescription, save as PDF format, and print report.&lt;/p&gt;
&lt;h2 id=&#34;android-applications&#34;&gt;Android Applications&lt;/h2&gt;
&lt;h3 id=&#34;1-doctors-window&#34;&gt;1. Doctor’s Window&lt;/h3&gt;
&lt;p&gt;This application runs on doctor’s mobile android phone or tab. Every doctor has create account with his/her email and user ID. When the doctor created an account with the system, he/she is taken to the profile update form. Then the doctor enters
his/her name, designation, specialty, institution, address, contact number and the visiting time to complete the profile. Once profile is completed he/she can login to the app and see his own patient’s list by date.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/sumonbis/DoctorsWindow/master/Android%20App/screens/Doctor-Side%20Application.png&#34; alt=&#34;Doctors side app&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;2-patients-window&#34;&gt;2. Patient’s Window&lt;/h3&gt;
&lt;p&gt;This application runs on the client’s smart devices to connect the desired doctor. No login or registration is required for this app. Anyone using this app will easily find different categories of specialty. Then he/she will be able to find doctors from different localities. The patient will choose doctor and see doctor’s detail informa- tion. Then he/she will click the “Get Appointment” button to seek for appointment. An automated scheduling system will run on the app server based on the doctor’s predefined time. This will check for free space and make the patient known about the appointment date and time. Eventually, the app will notify the patient before the appointment. Only one has to enter name, age and phone number for seeking appointment.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/sumonbis/DoctorsWindow/master/Android%20App/screens/Patient-Side%20Application.png&#34; alt=&#34;Patient side app&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tank Battle (Android Shooting Game)</title>
      <link>https://sumonbis.github.io/academic-project/tank-battle/</link>
      <pubDate>Tue, 18 Mar 2014 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/academic-project/tank-battle/</guid>
      <description>&lt;p&gt;“Tank Battle” is a game where you have to shoot at the enemy tanks at a high speed in different angle to destroy and continue. Enemies attack in a higher speed and try to enter passing the gamer’s tank. If any enemy tank passes you are defeated and the game is over. You can play again to try a higher score. This is a single player game and gives the gamer an immense pleasure and challenging environment. The attractive graphics and music takes to the real battlefield feel the fight.&lt;/p&gt;
&lt;p&gt;Features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Superb quality graphics&lt;/li&gt;
&lt;li&gt;Attractive sound&lt;/li&gt;
&lt;li&gt;Extremely addictive gameplay&lt;/li&gt;
&lt;li&gt;Enemies power up gradually&lt;/li&gt;
&lt;li&gt;Shoot, destroy thousands of tanks and enjoy&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;game-screens&#34;&gt;Game screens:&lt;/h2&gt;














&lt;figure  id=&#34;figure-home-screen&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/sumonbis/TankBattle/master/screens/1.png&#34; alt=&#34;Home Screen&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Home Screen
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-game-screen&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/sumonbis/TankBattle/master/screens/2.png&#34; alt=&#34;Game Screen&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Game Screen
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-score-screen&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/sumonbis/TankBattle/master/screens/3.png&#34; alt=&#34;Score Screen&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Score Screen
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://sumonbis.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
