<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Fairness | Sumon Biswas</title>
    <link>https://sumonbis.github.io/tag/fairness/</link>
      <atom:link href="https://sumonbis.github.io/tag/fairness/index.xml" rel="self" type="application/rss+xml" />
    <description>Fairness</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2021 Sumon Biswas</copyright><lastBuildDate>Sun, 14 May 2023 21:25:33 -0500</lastBuildDate>
    <image>
      <url>https://sumonbis.github.io/media/icon_hub4cc8572db2cf4087d387aa00fd8abd9_555_512x512_fill_lanczos_center_3.png</url>
      <title>Fairness</title>
      <link>https://sumonbis.github.io/tag/fairness/</link>
    </image>
    
    <item>
      <title>Fairify: Fairness Verification of Neural Networks</title>
      <link>https://sumonbis.github.io/publication/icse23a/</link>
      <pubDate>Sun, 14 May 2023 21:25:33 -0500</pubDate>
      <guid>https://sumonbis.github.io/publication/icse23a/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Towards Understanding Fairness and its Composition in Ensemble Machine Learning</title>
      <link>https://sumonbis.github.io/publication/icse23b/</link>
      <pubDate>Sun, 14 May 2023 21:20:33 -0500</pubDate>
      <guid>https://sumonbis.github.io/publication/icse23b/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Causal Fairness in Machine Learning Pipeline</title>
      <link>https://sumonbis.github.io/project/modular-fairness/</link>
      <pubDate>Fri, 28 May 2021 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/project/modular-fairness/</guid>
      <description>&lt;p&gt;Software fairness has been violated in many critical predictive applications in recent times. We have seen a number of those news in last few yers.
The machine learning (ML) models used to make the predictions can exhibit bias for various reasons. In this project, we address the &lt;em&gt;algorithmic fairness&lt;/em&gt; of the models, which is measured from the predictions of the model.&lt;/p&gt;














&lt;figure  id=&#34;figure-reported-fairness-violations&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Reported fairness violations&#34; srcset=&#34;
               /project/modular-fairness/news_hu3233d3db2e418be631d6910f666245d9_5012327_46019d95c58d1b31ccdfb7e1814ec458.jpg 400w,
               /project/modular-fairness/news_hu3233d3db2e418be631d6910f666245d9_5012327_2a36cd5627ba236ea243abbddeb4c04f.jpg 760w,
               /project/modular-fairness/news_hu3233d3db2e418be631d6910f666245d9_5012327_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://sumonbis.github.io/project/modular-fairness/news_hu3233d3db2e418be631d6910f666245d9_5012327_46019d95c58d1b31ccdfb7e1814ec458.jpg&#34;
               width=&#34;760&#34;
               height=&#34;390&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Reported fairness violations
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Many research looked at the problem and proposed different measures and mitigations to make the models fairer. However, the prior works consider the ML model wholistically as a black-box, and do not look at the fairness of components in the ML pipeline. ML pipeline can have several components and stages such as preprocessing, training, tuning, evaluation, etc. Each of them can affect the ultimate fairness of the model. Our goal is to investigate the fairness in the component-level and identify the modules that are causing the unfairness.&lt;/p&gt;














&lt;figure  id=&#34;figure-reported-fairness-violations&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Reported fairness violations&#34; srcset=&#34;
               /project/modular-fairness/black-box_hu309927eb576099c7084cf9aed61d13b6_83467_c2d7ec41a2a83694864295033989e2f5.png 400w,
               /project/modular-fairness/black-box_hu309927eb576099c7084cf9aed61d13b6_83467_586a4e5f594e218262325243c671596d.png 760w,
               /project/modular-fairness/black-box_hu309927eb576099c7084cf9aed61d13b6_83467_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sumonbis.github.io/project/modular-fairness/black-box_hu309927eb576099c7084cf9aed61d13b6_83467_c2d7ec41a2a83694864295033989e2f5.png&#34;
               width=&#34;432&#34;
               height=&#34;196&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Reported fairness violations
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;First, we do not consider the whole ML model as a single black box. Along with commenting on the fair of unfair behavior of the whole model, we look inside the black box and try to understand which components are responsible for the unfairness of the model. &lt;a href=&#34;https://sumonbis.github.io/publication/esec-fse21/&#34;&gt;Our FSE&#39;21 paper&lt;/a&gt; paper focused on identifying unfair preprocessing stages in ML pipeline.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    What is the fairness measure of a certain component/stage (e.g., imputation, scaling, etc.) in ML pipeline?
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Look at the following ML pipeline which is taken from the &lt;a href=&#34;https://github.com/propublica/compas-analysis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;crime prediction analysis repository of Propublica&lt;/a&gt;. The pipeline operates on Compas dataset that contains records of about 7k defendants in Florida. This was used at US courts in at least 10 states including New York, Wisconsin, California, Florida, etc &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. The pipeline transforms data using six data transformers before applying the LogisticRegression model. For example, in line 2-5, custom data filtration was applied, and in line 12, an imputation method from the library was applied to replace the missing values for the feature &lt;code&gt;is_recid&lt;/code&gt;. When we measure the fairness of this model using existing metrics such as statistical parity or equal opportunity, that does not say anything about the fairness of these data transformers.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;
&lt;table style=&#34;border-spacing:0;padding:0;margin:0;border:0;&#34;&gt;&lt;tr&gt;&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 1
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 2
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 3
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 4
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 5
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 6
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 7
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 8
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 9
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;10
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;11
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;12
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;13
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;14
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;15
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;16
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;17
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;18
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;;width:100%&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(f_path)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[(df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;days_b_screening_arrest &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; (df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;days_b_screening_arrest &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; (df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;is_recid &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; (df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;c_charge_degree &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;O&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; (df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;score_text &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;N/A&amp;#39;&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;replace(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Medium&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Low&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; LabelEncoder()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_transform(df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;score_text)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;impute1_onehot &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Pipeline([
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;imputer1&amp;#39;&lt;/span&gt;, SimpleImputer(strategy&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;most_frequent&amp;#39;&lt;/span&gt;)),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;onehot&amp;#39;&lt;/span&gt;, OneHotEncoder(handle_unknown&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ignore&amp;#39;&lt;/span&gt;))])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;impute2_bin &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Pipeline([
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;imputer2&amp;#39;&lt;/span&gt;, SimpleImputer(strategy&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;)),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;discretizer&amp;#39;&lt;/span&gt;, KBinsDiscretizer(n_bins&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, encode&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ordinal&amp;#39;&lt;/span&gt;, strategy&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;uniform&amp;#39;&lt;/span&gt;))])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;featurizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ColumnTransformer(transformers&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;impute1_onehot&amp;#39;&lt;/span&gt;, impute1_onehot, [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;is_recid&amp;#39;&lt;/span&gt;]),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;impute2_bin&amp;#39;&lt;/span&gt;, impute2_bin, [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;age&amp;#39;&lt;/span&gt;])])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Pipeline([(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;features&amp;#39;&lt;/span&gt;, featurizer),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;classifier&amp;#39;&lt;/span&gt;, LogisticRegression())])&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We used causal reasoning in software to identify the fairness impact of those stages in the prediction.&lt;/p&gt;
&lt;h2 id=&#34;causality-in-software&#34;&gt;Causality in Software&lt;/h2&gt;
&lt;p&gt;Identifying causal effects has been an integral part of scientific inquiry. It helped to answer a wide range of questions like - understanding behavior in online systems, or effect of social policies, or risk factors for diseases and so on.&lt;/p&gt;
&lt;p&gt;In causal testing, given a failing test, causal experiments are conducted to find a set of test-passing inputs that are close to the failing input.
In this project, we also used this casual modeling on the pipeline. We intervene on one variable of interest at a time and observe the change in the outcome.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    For two random variables $X$ and $Y$, we say that $X$ causes $Y$ when there exist at least two different interventions on $X$ that result in two different probability distributions of $Y$.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;causality-in-fairness&#34;&gt;Causality in Fairness&lt;/h2&gt;
&lt;p&gt;Causality in fairness has also been studied in the literature. “Other things being equal”, prediction would not have changed in the counterfactual world, where only the intervened variable would have changed.&lt;/p&gt;
&lt;p&gt;A predictor $\hat{Y}$ is said to satisfy causal fairness if&lt;/p&gt;
&lt;p&gt;$$
P(\hat{Y}(a, U) = y | X = x, A = a) = P(\hat{Y}(a&amp;rsquo;, U) = y | X = x, A = a)
$$&lt;/p&gt;
&lt;p&gt;We create an alternative pipeline $\mathcal{P}* $ from the given pipeline $\mathcal{P} $ by removing the preprocessing stage in consideration. Then we look at the prediction disparity between $\mathcal{P} $ and $\mathcal{P}* $. The disparity can be fairness satisfying or not. To evaluate that, we used the existing fairness criteria from the literature.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Thus, we proposed four novel metrics to measure fairness of a stage in the ML pipeline.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We observed a number of patterns of fairness of the the data transformers that are commonly used in pipelines.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Data filtering and missing value removal change the data distribution and hence introduce bias in ML pipeline.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;New feature generation or feature transformation can have large impact on fairness.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Encoding techniques should be chosen cautiosly based on the classifier.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Similar to the tradeoff between the accuracy and fairness for the classifier, the stages of the pipelines also exhibit the tradeoff. Often the accuracy-improve data transformer is unfair.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Among all the transformers, applying sampling technique exhibits most unfairness.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Selecting a subset of features often increase unfairness.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Feature standardization and non-linear transformers are fair transformers.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Furthermore, another impact that our method could attain is that we can instrument the pipeline. A pipeline can have a unfair stage that favors the privileged. Similarly, there can be another stage that favors the unprivileged. Both stages can be used in a pipeline such that their unfairness is canceled.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    The fairness composition can help to choose appropriate alternatives while development and improve the overall fairness.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We noticed that the most popular fairness packages (e.g., &lt;a href=&#34;https://aif360.readthedocs.io/en/latest/#&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AIF360&lt;/a&gt;) uses a default data preprocessing each time users import datasets from the packages. There is no option to control or measure the unfair stages in the pipeline. Our early results would provide guidance to analyze fairness at a component-level. Further research in the area is in progress to understand fairness composition and optimize the pipeline construction.&lt;/p&gt;
&lt;!-- 






  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span class=&#34;author-highlighted&#34;&gt;
      Sumon Biswas&lt;/span&gt;, &lt;span &gt;
      Hridesh Rajan&lt;/span&gt;
  &lt;/span&gt;
  &lt;div&gt; 
    &lt;a href=&#34;https://sumonbis.github.io/publication/esec-fse21/&#34;&gt;Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline&lt;/a&gt;
  &lt;/div&gt;
  In 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE), Athens, Greece,
  2021.

  
  &lt;div class=&#34;btn-links&#34;&gt;
    








  





&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/esec-fse21/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/sumonbis/FairPreprocessing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1145/3468264.3468536&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


  
  
  
    
  
  
  
  
  
    
  
  &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://design.cs.iastate.edu/papers/ESEC-FSE-21/fair-preprocessing-fse21.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
    &lt;i class=&#34;fas fa-file-pdf mr-1&#34;&gt;&lt;/i&gt;
    PDF
  &lt;/a&gt;

  
  
  
  
  
  
  
    
  
  &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/2106.06054&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
    &lt;i class=&#34;ai ai-arxiv mr-1&#34;&gt;&lt;/i&gt;
    arXiv
  &lt;/a&gt;

  
  
  
    
  
  
  
  
  
    
  
  &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://youtu.be/X-Nvn6DhHsA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
    &lt;i class=&#34;fab fa-youtube mr-1&#34;&gt;&lt;/i&gt;
    Talk
  &lt;/a&gt;


  &lt;/div&gt;
  

  
&lt;/div&gt;

  

 --&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline</title>
      <link>https://sumonbis.github.io/publication/esec-fse21/</link>
      <pubDate>Thu, 20 May 2021 21:25:33 -0500</pubDate>
      <guid>https://sumonbis.github.io/publication/esec-fse21/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Our Research Identifies Unfairness in the Component Level of AI Based Software</title>
      <link>https://sumonbis.github.io/post/fair-pipeline/</link>
      <pubDate>Sun, 02 May 2021 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/post/fair-pipeline/</guid>
      <description>&lt;p&gt;The government, academia, industry are increasingly employing artificial intelligence (AI) systems in decision making. With the availability of numerous data, AI systems are becoming more popular in various sectors. Many of these systems affect human lives directly in one way or another. Our research highlighted that many of such real-world machine learning (ML) models exhibit unfairness with respect to certain societal groups of race, sex or age. In the last few years, our software design lab employed significant effort to identify fairness in machine learning algorithms and mitigate that effectively. Recent result shows that several components in an ML pipeline are influencing the predictive result that is unfair to minority groups such as dark-skinned people or female.&lt;/p&gt;
&lt;p&gt;I and my advisor Hridesh Rajan are working in the D4 Institute at Iowa State which broadly focuses on increasing the dependability of AI-based systems.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The accuracy of a model is not always telling the whole story. How much bias the model propagates or how much we can trust the prediction is a big question.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;These AI-based software are being used in hiring employees, approving loans, criminal sentencing, which should be more accountable and explainable. Analyzing the behavior of ML pipeline in component level would help towards that goal.
&lt;a href=&#34;https://sumonbis.github.io/publication/esec-fse21/&#34;&gt;Our paper proposing a novel method to identify unfairness of ML components&lt;/a&gt; has been recently accepted in &lt;a href=&#34;https://2021.esec-fse.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ESEC/FSE 2021&lt;/a&gt;, which is one of top software engineering conference and internationally renowned forum for researchers, practitioners, and educators.&lt;/p&gt;
&lt;p&gt;In the paper, we proposed a causal method to measure the fairness of different stages in ML pipeline.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Although some recent work proposed metrics to quantify the bias in the predictions, ML software is complex having several stages. Our method could effectively identify the data transformers that caused unfairness in such software.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We strongly believe that the researchers and practitioners would be able to leverage our approach to avoid biased data preprocessing. Our goal in the long-run would be to unveil the ML black box and reason about fairness constraints.&lt;/p&gt;
&lt;p&gt;I will present the results of the paper entitled “Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline”, in the research track of the 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) to be held in Athens, Greece from August 23-28, 2021. The preprint of the paper is available here: &lt;a href=&#34;https://arxiv.org/pdf/2106.06054.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/2106.06054.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We are excited on the acceptance of the paper in a top venue in our area. We are continuing the research to explore new avenues and assure the fairness of machine learning software.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Do the Machine Learning Models on a Crowd Sourced Platform Exhibit Bias? An Empirical Study on Model Fairness</title>
      <link>https://sumonbis.github.io/publication/esec-fse20/</link>
      <pubDate>Sun, 08 Nov 2020 21:25:33 -0500</pubDate>
      <guid>https://sumonbis.github.io/publication/esec-fse20/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Fairness Engineering in ML Models</title>
      <link>https://sumonbis.github.io/project/empirical-fairness/</link>
      <pubDate>Tue, 19 May 2020 00:00:00 +0000</pubDate>
      <guid>https://sumonbis.github.io/project/empirical-fairness/</guid>
      <description>&lt;p&gt;Software engineering (SE) research has focused on identifying, localizing, and repairing defects in software systems. With the evolution of programming languages, coding practices and requirements, various kinds of bugs of software has drawn interest. Recent trend of machine learning techniques has raised the question of dependability of the predictive decisions. Fairness of the machine learning (ML) models has been an open issue for such software systems in last few years. In critical decision making such as loan approval, crime prediction, college admission or hiring employees, ML algorithms are being used. However, the problem of fairness has not been studied by the SE community as much as the ML community looked into it. We conducted an empirical study in &lt;a href=&#34;https://sumonbis.github.io/publication/esec-fse20/&#34;&gt;our FSE&#39;20 paper&lt;/a&gt; to investigate a number of SE concerns of measures, mitigation, and impact of unfairness in ML models.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It is important to ensure the fairness of the ML models so that no discrimination is made based on protected attribute (e.g., race, sex, age) while decision making.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;First, it is important to look at the real-world software systems to understand the state of unfairness. Most of the study experiment using default-set classifiers from libraries. Thus, often the real faults and concerns with fairness are missed. However, it might be difficult to find such open-source models in the wild. We found a good number of models from Kaggle that helped to point down fairness issues in the wild.&lt;/p&gt;
&lt;h2 id=&#34;fairness-measures&#34;&gt;Fairness Measures&lt;/h2&gt;
&lt;p&gt;The foremost big concern is that how do we know there is a fairness issue in my model. Well, it depends on the stakeholder and the context. The participants could ask for &lt;em&gt;equal opportunity&lt;/em&gt;, while the owners focus on the &lt;em&gt;disparate impact&lt;/em&gt;. A popular library for fairness, &lt;a href=&#34;https://aif360.readthedocs.io/en/latest/#&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AIF360&lt;/a&gt; has more than 70 metrics to measure fairness. Two most common used broad categories of fairness measures are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Group fairness&lt;/li&gt;
&lt;li&gt;Individual fairness&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;em&gt;impossibility theorem&lt;/em&gt; of fairness suggests that satisfying fairness with respect to all the definition can not be achievable, since one definition can contradict another. Furthermore, fairness in classification problem has been focused is most of the works, while fairness of regression or ranking problems is not yet well studied.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Choosing appropriate fairness metrics and optimizing them simultaneously is an important requirement engineering problem to be solved.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We found several interesting finding from our experiments:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Only one fairness metric would not show the whole picture. There is no metric that combines multiples metric too.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fairness metrics have their assumptions as well. Most of the group fairness metrics require the dataset divided into privileged and non-privileged. So, it can not compute unfairness among multiple groups (e.g., Asian, American, African, etc.) at the same time. Furthermore, the metrics measure fairness with respect to one protected attribute (&lt;em&gt;race&lt;/em&gt; or &lt;em&gt;sex&lt;/em&gt; or &lt;em&gt;age&lt;/em&gt;). A fair model with respect to &lt;em&gt;sex&lt;/em&gt; could be biased with respect to &lt;em&gt;age&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Developers often tend to optimize the models for accuracy, which causes unfairness.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The library APIs such as classifiers from &lt;a href=&#34;https://scikit-learn.org/stable/modules/classes.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scikit-Learn&lt;/a&gt; have several constructs that affect fairness. But those are not document and developers are aware of their fairness impacts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is often thought that if data is fair, the model would be so too. &lt;mark&gt;Although data has a great deal of impact of fairness of the decisions, a model could be fairer which used biased data&lt;/mark&gt;. At the same time, a model could be biased which was trained on unbiased data. We found that some data preparation techniques, thus, can have both positive or negative impact on fairness. Specifically, the data standardization and feature engineering (removal or creation) techniques affect fairness almost always.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;bias-mitigation&#34;&gt;Bias Mitigation&lt;/h2&gt;
&lt;p&gt;There are many bias mitigation techniques proposed in the literature, which are grouped into these categories:&lt;/p&gt;
&lt;!-- {style=&#34;color: red&#34;} --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Preprocessing techniques:&lt;/strong&gt; These techniques operate on the training data to make the resulting model fairer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;In-processing techniques:&lt;/strong&gt; These techniques alters the existing mode.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Post-processing techniques:&lt;/strong&gt; The predictions are changes with some constraints to make them fairer.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Choosing a bias mitigation technique can be also confusing. Applying one mitigation technique can work for a particular metric and hamper accuracy as well as another fairness metric.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In our paper, we reported several observations that help to choose a mitigation technique for the model.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Effective preprocessing mitigation technique is preferable, since it does not ruin accuracy in most of the cases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the model exhibits a lot of bias, post-processing techniques are the most successful.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Originally fairer models are debiased effectively by preprocessing or in-processing
techniques.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;impacts&#34;&gt;Impacts&lt;/h2&gt;
&lt;p&gt;Fairnes does not come free. In most of models, there is a accuracy loss when the mitigation technique are applied. Multi-objective optimization could be utilized to improve fairness and accuracy together.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;In-processing mitigation algorithms show uncertain behavior in their performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Although post-processing algorithms are the most dominating in debiasing, they are always diminishing the model accuracy and F1 score.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Trade-off between performance and fairness exists.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Overall, ensuring fairness require to address many dimensions of the problem at the same time. There is a wrong perception that only debiasing training data would suffice to make the model fair. The training is not always uniform and it does not learn from all the data instances equally. So, its needed to make the learning process better as well. Because of the tradeoff, multiple metrics, domain-specific issues, we need further testing and verification mechanisms as well. A few studies proposed fairness aware language constructs and test input generation techniques. However, we need further languages, tools, and methods to localize the fairness issue and repair them in real-world situations.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
